> 아무튼 어두컴컴하고 축축한데서 야옹야옹 울고있었던 것만은 분명히 기억한다 - 나쓰메 소세키,

지금까지의 신경망은 feed forward 유형의 신경망이다.
**feed forward란 흐름이 단방향**인것을 말한다.

feedforward 신경망은 단순하여 많은 문제에 응용이 가능하지만, **_시계열 데이터를 잘 다루지 못하는 단_**점이 있다.(충분히 학습하지 못한다)

==> 순환 신경망(Recurrent Neural Network, RNN)이 등장

# 5.1.1 word2vec의 CBOW 모델 복습

w1,w2,w3와 같은 단어열로 표현되는 말뭉치가 있을때 t번째 단어를 타깃, 앞 뒤의 단어를 맥락으로 이해

이때 CBOW모델은 P(Wt|Wt-1, Wt+1)의 사후 확률을 모델링한다.==> 즉 Wt-1 과 Wt+1이 주어졌을대 Wt 가 일어날 확률이고, 이것이 window크기가 1일때 CBOW 모델

이때 맥락의 윈도우를 양방향으로 균일하게 안 주고 한쪽으로 비대칭적이게 줄수도 있다. 이때도 손실함수의 총합을 최소화 하는 방향으로 가중치 매개변수를 찾아나가게 된다.

그리고 이러한 목적을 통해서 타깃을 정확하게 예측할 수 있게 되고, 부가적으ㅗ 단어의 의미가 인코딩 된 단어 분산표현을 얻을수 있다.

# 5.1.2 언어 모델

언어 모델(Language Model)은 단어의 나열에 확률을 부여하는데,(단어 하나하나가 아닌 나열 자체에 확률을 부여)

따라서 얼마나 자연스러운 단어순서인지를 평가하게 된다.

eg. 오늘 저녁은 돈까스를 먹자 : 0.1의 확률
    오늘 저녁은 휴대폰을 먹자 : 0.00001으 확률


언어모델은 기계번역 or 음성인식에서 응용할 수 있는데,
음성인식의 경우 문장을 후보로 생성하고 언어모델을 활용해서 각 문장의 우선순위를 정할 수 있게 된다.

뿐만아니라 새로운 문장을 생성하는 용도로 활용 할 수도 있다. 확률분포에 따라 다음으로 적합한 단어를 생성할수 있기 때문,

> P(A,B) = P(A|B)P(B) : 곱셈정리
> A와 B가 모두 일어날 확률 은 B가 일어날 확률과 B가 일어나고 A가 일어날 확률을 곱한값이다.

동시성 때문에 곱하기가 들어간다.

우리가 언어 모델에서 다루는 사후 확률또한 특정 문장에서 많은 단어들이 동시에 일어나야하므로 곱셈이들어가는데 이때 사후 확률에서 눈여겨 봐야할 부분은 t 번째 단어를 타깃으로 했을때 t왼쪽에 있는 단어들을 앞에서부터 순서대로 맥락으로 고려해서 계산을 해줘야 한다는 것이다

### 5.1.3 CBOW모델을 언어 모델로하면?

> 마르코프 연쇄 or 마르코프 모델이라는 말이 있는데, 미래의 상태가 현재상태에만 의존해 결정되는 것으로 N층 마르코프 연쇄라고 하면 직전의 N개 영향을 받는다는 것이다.

타겟을 결정할 수 있는 맥락의 크기는 임의로 설정할 수 있는데, 이때 임의로 설정해도 특정 길이로 고정된다.
(이런것은 보다 over 범위에 맥락을 파악해야하는 단어가있을때 문제가 된다.)

CBOW 모델에서는 맥락을 키운다하더라도 문제가 있는데, 바로 단어의 순서를 무시하는 한계가 문제이다.

> CBOW : continuous bag of words 의 약자로 가방안의 단어는 순서 없이 막들어 있는 것처럼 순서가 없고, 분포를 이용하는게 CBOW이다.

CBOW모델은 (you,say)와 (say,you)의 맥락을 똑같이 취급하기 때문에 단어벡터를 더하는 방식을 취하기 보다 단어벡터들을 인풋 은닉층에서 연결하는 방식으로 생각을 해보면 좋지않을까? (이런 모델이 신경 확률론적 언어모델에서 제안한 모델이다) 하지만 이렇게 되면 가중치 매개변수또한 늘어나게되어 별로 좋지 못하다.

==> 따라서 RNN을 이용하면 되는데, RNN은 맥락이 아무리 길어도 맥락의 정보를 기억하는 매커니즘을 가지고 있다. 즉 아무리 긴 시계열 DATA라도 대응할수 있다.


> word2vec 은 단어의 분산표현을 얻기위해서 고안된 기법,
> 따라서 이를 언어 모델로 사용하는 경우는 잘 없고 
> 우리는 RNN의 강점을 더 잘 느끼기 위해ㅜword2vec의 CBOW모델을 언어모델에 적용해봤다
> RNN을 통해서도 분산표현을 얻을수 있지만, 단어수 증가에따른 대응이나 질 개선을 위해 word2vec을 제안함.

## RNN

Recurrent Neural Network 로 몇번이나 반복해서 일어나는 일을 말한다.
즉 순환하는 신경망이란 뜻이다.

>Recursive Neural Network 재귀 신경망도 있는데  tree 구조의 data를 처리하기 위한 신경망으로 RNN과 구분을 해야한다.

### 5.2.1 순환하는 신경망

순환하기위해서는 경로가 닫혀있어야한다.
그리고 이를 통해서 과거의 정보를 기억함과 동시에 갱신할 수도 있다.

Xt(시계열데이터) ---> RNN -----> ht
(이때 RNN--> ht로 나가는 결과값중 일부가 분기해 RNN으로 재입력)

### 5.2.2 순환 구조 펼치기
       
이러한 순환구조를 펼치게 되면 X1에서 RNN으로 입력해서 나오는 h1의 분기값이 다시 X2의 RNN에 같이 입력이 되고.. 이런식으로 다음 시각t의 RNN계층에 영향을 주게 된다.

이러한 결과로 현시각의 출력은

`ht = tanh(ht-1Wh+xtWt + b)`라는 복잡한 식이 나오게 된다.

RNN에서는 가중치가 2개 있는데 하나는 입력 x를 출력 h로 변환하기 위한 가중치 Wx, 다른 하나는 1개의 RNN출력을 다음 시각의 출력으로 변환하기위한 가중치 Wh이다.

또한 편향 b도 있고, bt-1과 xt는행벡터이다.

무엇보다도 중요한것은 ht는 h(t-1)을 기초해 계산하므로
RNN은 h라는 상태를 가지고 있고, 때문에 메모리가 있는 계층이라고도 한다.

> 많은 책에서 RNN의 출력 ht를 은닉상태 or 은닉상태벡터라고 하기에 이 책에서도 RNN의 출력 ht를 은닉상태벡터라고 부르자.

### 5.2.3 BPTT
RNN계층을 가로로 펼친 신경망으로 간주하게 되면 오차역전파법도 적용할수 있게 되는데, 순전파--> 역전파를 수행하여 원하는 기울기를 구할 수 있다.

이때의 오차역전파법은 시간방향으로 펼친 신경망의 오차역전파법의 의미로 (Backpropagation Through Time, BPTT라고 한다) 

BPTT를 이용하면 RNN을 학습할수 있을 것 같은데, 긴 시계열을 학습할때 문제가 될수 있다.

시계열 data가 커질수록 BPTT가 소비하는 컴퓨팅 자원도 늘어나고, 역전파시의 기울이가 불안정해지는 것도 문제다.
(BPTT는 매시각 RNN계층의 중간 data를 mem에 유지해둬야해서 CPU뿐만아니라 mem사용량도 즐가 한다.)


### 5.2.4 Truncated BPTT

따라서 너무 큰 data는 적당길이로 끊는다.
끊어서 작은 신경망 단위로 오차역전파법을 수행한다.

(제대로 구현하려면 순전파는 그대로 유지하되 역전파의 연결을 적당한 길이로 잘라내야한다.-> 역전파의 연결을 잘라버리면 그보다 미래의 data에 대해서는 생각할 필요없이 독립적으로 오차역전파법을 완결시켜 주면 된다.)

>이때 반드시 기억해야할 것은 역전파의 연결은 끊어지는 반면 순전파의 연결은 끊어지지 않기에 RNN을 학습시킬때는 순전파가 연결되는 점을 고려해 순서대로 입력해야한다.


### 5.2.5 Truncated BPTT의 미니배치학습

eg) 길이가 1000인 시계열 data에 대해서 시각의 길이를 10개 단위로 잘라서 BPTT를 학습하는 경우를 예로 들어보자
그리고 이때 미니배치수를 2개로 구성해서 학습을 하려면 
첫번째 미니배치는 0번부터 10개단위로 시작하게 되고 두번째 미니배치는 500번째 data를 시작위치로 정하고 시작하게 된다.
그리고 순서대로 data를 입력하다가 끝에 도달하면 다시 청므부터 입력하도록 한다.


### RNN 구현

RNN 은 특정 시각 ~ 시각까지의 범위를 모듈화 시켜서 구현을 할수 있다.

그리고 역전파에서는 분기되는 h덕분에 다시 역전파로 돌아올때 +를 해서 역전파를 전해주어야한다.

## 시계열 데이터 처리 계층 구현

RNN을 사용하여 언어 모델을 구현하는 것이 목표이다.
RNN계층(과 시계열 data를 한꺼번에 처리하는 TimeRNN계층)을 구현했는데, 시계열 데이터를 처리하는 계층을 몇개 더 만들어 보자.

RNN을 사용한 언어모델은 영어로 RNN Model이므로 RNNLM으로 이야기하자

RNN을 사용해서 학습을 한다 하더라도 입력값에 대해서 첫번째로 Embedding 계층을 통과하여 단어 ID를 단어 벡터로 변환해주고 그 분산표현을 RNN계층으로 
입력해준다음 출력되는 값을 다음 RNN과, Affine계층->Softmax계층으로 보내준다.

이러한 과정을 통해서 RNN계층은 과거에서 현재로 계속 흘러들어오는 정보를 인코딩해 저장할수 있는 것이다.

### 언어 모델의 평가

언어몯레은 주어진 과거 단어로 부터 다음에 출현할 단어의 확률분포를 출력한다
이때 언어 모델의 예측성능을 평가하는 척도로 퍼플렉서티(perplexity,혼란도) 를 자주 이용한다

간단히 확률의 역수로 해당 값이 작을수록 더 정확하다는 뜻이다
(eg. 확률이 0.8일때와 0.2일때의 역수를 각각 비굫보면 1.25와 5이다.)

이를 우리는 분기수(다음에 취할수있는 선택사항의 수, 다음으로 올수있는 단어의 후보 수)로 해석할 수 있고 그렇게 위의 숫자를 해석하면 된다.

> 정보이론 분야에서는 퍼플렉서티를 기하평균 분기수라고도 한다.

데이터 제공방법과 퍼플렉서티 계산 부분이 지금까지와의 학습코드와 차이점을 보인다.

#### 데이터 제공방법

Truncated BPTT의 방법으로 학습을 수행하다보니 data를 순차적으로 주고 각각의 미니배치에서 데이터를 읽는 시작위치를 조정해야한다.
따라서 각 미니배치가 data를 읽기 시작하는 위치를 계산해 offsets에 저장해둔다.

데이터를 순차적으로 읽으며 그릇(배치)을 준비하고 오프셋을 이용하여 미니배치에서 오프셋을 추가한다.

#### 퍼플렉서티
에폭마다 퍼플렉시티를 구하기위해 에폭마다 손실의 평균을 구하고, 그 값을 이용해 구한다.


## 5.6 정리

RNN은 데이터를 순환함으로써 과거-현재 그리고 미래로 데이터를 계쏙 흘려보낸다
그리고 이를 위해 은닉상태를 기억하는 능력이 추가되었다.

뿐만아니라 RNN을 이용해서 언어모델을 만들고 단어시퀀스에 확률을 부여하여
다음에 출연할 단어의 확률을 계산해준다.

따라서 RNN을 이용한 신경망 구성이 등장하고 아무리 긴 시계열 DATA도 RNN의 은닉상태에 기억해둘수 있지만 실제문제에서는 잘 학습하지 못하는 경우가 많다

따라서 이런 문제를 해결하기 위해 LSTM, GRU계층을 앞으로 살펴보자
