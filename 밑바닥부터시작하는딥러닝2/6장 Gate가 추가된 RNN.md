> 망각은 더 나은 전진을 낳는다.

# Gate 가 추가된 RNN

앞에서 본 RNN은 순환 경로를 포함해 과거의 정보를 기억할수 있지만 장기 의존관계를 잘 학습할수 없기에 성능이 좋지 못하다

따라서 RNN 대신 LSTM(Long Short-Term Memory) 이나 GRU라는 계층이 주로 사용되고 RNN이라고 하면 LSTM을 가리키는 경우가 많다.
LSTM이나 GRU는 gate라는 구조가 추가적으로 잇는데, 이 게이트 덕분에 시계열 데이터의 장기적 의존관계 학습이 가능해진다.

## 6.1 RNN의 문제점

RNN의 장기의존관계 학습이 어려운 이유는 BPTT에서 기울기소실 or 기울기 폭발이 일어난다

### 6.1.1 RNN 복습

5장을 보고 오자

### 6.1.2 기울기 소실 또는 기울기 폭발

RNN계층이 과거방향으로 기울기를 전달하는데 이 기울기가 중간에 소실이 되면(정보가 사라짐) 가중치 매개변수는 갱신되지않고, 장기의존관계를 학습할수 없게된다.

### 6.1.3 기울기 소실과 기울기 폭발의 원인

RNN에서 기울기 소실 or 폭발이 일어나는 원인은 무엇일까?

역전파가 전해지면서 tanh와 MAtMul을 거치면서 작아진다.
tanh의 경우에는 미분을 하게되면 x=0을 기준으로 멀어지면 작아지기에 노드에서 멀어질수록 작아진다는것,

> RNN계층의 활성화 함수로는 tanh를 사용하는데 ReLU로 바꾸면 기울기 소실을 줄일수 있다.

반대로 MatMul을 이용하게 되면 기울기 폭발이 발생하게 되는데, 이러면 결국 overflow가 발생해 NaN과 같은 값을 가진다.

이때의 변화들은 지수적으로 증가, 감소하는데 그 이유는 T번만큼 곱해지기에 exponential하게 증가 감소하는 것이다.

만약 행렬이라 하더라도 그 행렬의 특잇값(퍼져있는 정도)이 척도가 되는데 1보다 큰지 작은지를 보면 어떻게 변할지 약간의 예측은 된다.

### 6.1.4 기울기 폭발 대책

기울기 폭발대책으로 기울기 클리핑이라는 기법이 있다.
무슨느낌이냐면 약간 기울기가 문턱값이라고 하는 값을 넘어서면 수식의 기울기를 조정하여 수정한다.
일반적으로 이때의 기울기는 신경망의 모든 매개변수의 기울기를 합한 값을 말한다.

## 6.2 기울기 소실과 LSTM

RNN 학습에서는 기울기 소실이 큰문제인데, 이를 해결하기위해서는 아키텍쳐를 근본부터 뜯어고쳐야한다.

따라서 gate가 추가된 RNN을 이용해서 해결해보자

### 6.2.1 LSTM의 인터페이스

LSTM계층의 인터페이스에서는 c라는 경로가 추가로 있다는 차이가 있는데 이를 memory cell(그냥 cell이라고도)하며 LSTM 전용 기억 메커니즘이다.

- 기억셀의 특징은 데이터르 자기 자신으로만(LSTM 계층 내에서만) 주고 받는 다는 것이다.즉, LSTM계층 내에서만 완결되고, 다른 계층으로는 출력하지 않는다. ==> 외부에서는 c가 안보인다.

- 은닉상태 h는 RNN계층과 마찬가지로 다른 계층으로 출력된다.

### 6.2.2 LSTM 계층 조립하기

> Understanding LSTM Netwroks 블로그를 참고해서 썼다.

c는 c와 h그리고 x로 부터 어떤 계산을 해서 구할수 있는데,
핵심은 갱신된 c를 통해 은닉상태 h를 계산하는 것이다.
`h = tanh(c)` 인 만큼 c의 각 요소에 tanh함수를 적용한다는 뜻이다.
따라서 기억셀 c와 은닉상태 h의 원소수가 같다는 말이다.

> 게이트란? 말그대로 수문을 열듯이 흐르는 양을 조절할 수 있는 것인데, 열고 닫는 것 뿐만아니라 흐르는 양도 조절하고, 심지어 이는 데이터로 부터 자동으로 학습한다.(0.0~1.0, 따라서 시그모이드 함수 이용)

### 6.2.3 output gate

output gate라는 말은 다음 은닉상태 h의 출력을 담당하는 gate라는 의미이다.
따라서 우리는 tanh가 이를 담다아므로 outputgate라고 해주겠다.
`o = б(x(t)W^(o)x + h(t-1)W^(o)h + b(o))`
`h = o ⨀ tanh(c)` 여기서 o는 x와 h에 각각 가중치를 곱해 편향을 더한 후 시그모이드 함수를 거친 출력이고, ⨀는 아다마르 곱(원소별 곱)이라고 해준다.

여기까지가 LSTM의 outputgate이다.

> tanh의 값은 -1.0~1.0의 수치이고, 인코딩된 정보의 강약을 나타내는데, sigmoid함수의 출력은 0.0~1.0 사이의 실수이고, 데이터를 얼마나 통과시킬지 정하는 비율이다.

따라서 gate에서는 sigmoid함수가, 실제 정보를 지니는 data에는 tanh함수가 활성화 함수가 된다.

### 6.2.4 forget 게이트

> 망각은 더 나은 전진을 낳는다.

우리가 다음으로 신경써야할 부분은 c(기억 셀)에 무엇을 잊을까 지시하는 것이다.

forget gate를 추가해보자

forget gate는 들어오는 h(t-1) 과 xt (그리고 편향)을 받아서 시그모이드함수를 거쳐 f를 반환하는데 `c(t) = f⨀c(t-1)` 이런식으로 c에 영향을 준다.

### 6.2.5 새로운 기억셀

잊는 것뿐만 아니라 새로 기억을 할수도 있어야하는데, 그러기 위해서는tanh노드를 추가하자.

tanh노드는 gate가 아니고, 새로운 정보를 기억셀(c)에 추가하는게 목적이다.

따라서 f 로 잊어버릴 정보를 c(t-1)에 연산을 해준다음
g, 즉 기억해야할 정보를 c(t-1)에 추가적으로 연산을 해주고
tanh를 지나
그다음 우리의 outputgate를 지난 값과 함께 연산을 해주게 된다.

### 6.2.6 input 게이트

g게이트 즉, 새로운 기억셀에 추가되는 정보의 가치가 얼마나 큰지를 판단하게 되고, 적절히 취사선택 가능하게 해준다.
따라서 g gate 새로운 기억셀이 추가될때 inputgate를 거친 값이 같이 연산되어서 c에 반영된다

### 6.2.7 LSTM의 기울기 흐름

LSTM의 역전파 흐름도 +와 x가 있는데, +는 그냥 그대로 전달을 하고, x가 문제인데 LSTM에서는 행렬의 곱이 아닌 원소별 곱이 이뤄지고, 매시각 다른 gate값을 이용해서 원소별 곱을 계산하므로 곱셈의 효과가 누적되지 않아 기울기 소실이 일어나지 않는다(어렵다).

곱 노드의 계산은 forget gate가 제어하는데, forget gate가 잊어야한다고 판단하면 기울기가 작아지고 아니면 보존된다.

## 6.3 LSTM 구현

LSTM의 각 gate및 노드를 구성하는 식을 계산에는 공통적으로
Affine transformation이 들어가있다.

> Affine Transformation 이란 행렬변환과 평행이동(편향)을 결합한 형태를 말한다

기존에 게이트마다 흩어져있던 가중치들을 큰 행렬의 형태로 하나로 모을수 있고, 그러면 개별적으로 진행되던 계산이 단 1회의 계산으로 끝마칠 수 있게된다.

> Slice 노드의 순전파와 역전파
> slice노드는 들어오는 행렬을 네조각으로 나누어 분배하는데, 역전파는 반대로 합쳐주는 역할을 한다.
> 이를 np.hstack()을 이용해서 구현해줄수 있다.

### 6.3.1 Time LSTM 구현

Time LSTM은 t개분의 시계열 데이터를 한번에 처리할수 있는 LSTM이다.

RNN학습을 할때는 Truncated BPTT를 수행해서 적당한 길이로 역전파의 연결을 끊었는데

우리는 LSTM을 구현할떄도 순전파의 흐름에서 은닉셀과 기억셀을 인스턴스 변수로 유지하면서 다음 forward가 불려도 이전 시각의 은닉상태와 기억 셀을 사용할 수 있다.

구현된 LSTM계층은 common/time_layer.py를 참고하자

## 6.4 LSTM을 사용한 언어모델

LSTM계층은 다 구현했으니 언어모델을 구현해보자.

RNN과 차이는 계층으로 LSTM을 사용한다는 것 밖에 없다.

> ch06/rnnlm.py를 참고하면 된다

> save_params 나 load_params 같은 method는 common/base_model.py의 BaseModel Class를 상속해서 사용하면 쉽게 기능을 사용할수있따

퍼플렉서티가 10000개라는 것은 다음으로 나올 단어의 후보를 1만개 정도로 좁혔다는 것을 뜻한다.

## 6.5 RNNLM 추가개선

RNNLM으로 정확한 모델을 만들고자 한다면 LSTM계층을 깊게쌓아서(여러겹)효과를 볼수있다.

LSTM계층을 2층 3층으로 여러겹 쌓으면 언어모델의 정확도 향상을 기대할수 있다.

단 쌓는 층수는 하이퍼 파라미터이므로 문제의 복잡도, 학습 data의 양에따라 적절하게 결정하는게 좋다.

PTB dataset의 경우에는 LSTM의 층수를 2~4정도로 하는게 좋다

> 구글 번역에서 쓰는 GNMT는 LSTM을 8층 쓴 신경망이다. 즉 처리문제가 복잡하고 data가 대량이라면 LSTM을 깊게 쌓는것이 정확도 향상에 좋다.

### 6.5.2 드롭아웃에 의한 과적합 억제

LSTM 계층을 다층화하면 시계열 data의 복잡한 의존관계를 학습할수 있을 것 같다.
즉 층이 깊어지면서 표현력이 풍부한 모델을 만들수 있는 것인데, 이런 모델은 과적합(overfitting)을 일으킨다.
특히 불행하게도 RNN은 일반적인 feed포워드 신경망보다 과적합이 쉽게 일어난다.(이에 대한 대책은 현재도 활발히 연구되고 있다.)

> 과적합이란 훈련데이터에 너무 치중된 나머지 일반화능력이 결여된상태,

과적합을 억헤난 방법으로

- 훈련데이터의 양늘리기
- 모델의 복잡도 줄이기

- 모델의 복잡도에 패널티를 주는 정규화도 효과적이다.(L2정규화는 가중치가 너무 커지면 패널티 부과)
- Dropout처럼 훈련시 계층내의 뉴런 몇개를 무작위로 무시하고 학습.(일종의 정규화)(무시 == 앞계층으로의 신호전달을 막는다는 말)

그렇다면 Dropout 계층은 어디에 삽입해야할까?

LSTM의 시계열방향으로 삽입을 하는 방법이 있지만 이는 좋은 방법이 아니다. 왜냐면 시간이 흐름에 따라 드롭아웃에 의한 노이즈가 축적되기 때문이다.

따라서 좋은 예시는 Dropout계층을 상하 방향으로 삽입하는게 좋다.
(그러면 시간이 아무리 흘러도 정보를 잃지 않는다.)

> 하지만 최근에는 RNN의 시간방향 정규화를 목표로 여러 방법이 제안되고, 변형 드롭아웃등의 방법들이 제안되고있다. 변형드롭아웃은 시간방향에도 적용할 수 있어서 모델의 정확도를 더 향상시킬수 있다.(mask라는 것을 공유함으로써, mask란 data의 통과/차단을 결정하는 이진형태의 무작위 패턴.)==> 잃게 되는 정보방법도 고정이되므로 정보가 지수적으로 손실되는 사태를 피할수 있다.

### 6.5.3 가중치 공류

가중치 공유(eg. embedding 계층과 softmax앞의 affine계층이 가중치 공유)
두 계층이 가중치를 공유함으로써 학습하는 매개변수 수도 줄어드는 동시에 정확도도 향상되는 기술,

embedding계층의 가중치 형상이 (어휘수X은닉상태차원수)의 모양을 가지면 Affine계층의 가중치 형상은 (은닉상태차원수X어휘수)의 모양을 가지기때문에 전치하여서 가중치를 공유해주면 된다.

> 가중치를 공유함으로써 학습해야할 매개변수를 줄이고, 학습이 쉬워지는데.
> 매개변수가 줄어든다는 것은 과적합이 억제되는 혜택으로 이어질 수 있다.

### 6.5.4 개선된 RNNLM 구현

- LSTM계층의 다층화(여러 층 구현(여기에서는 2층))
- 드롭아웃 사용(깊이 방향으로 적용)
- 가중치 공유(Embedding계층과 Affine계층에서 가중치 공유)

매 Epoch에서 검증데이터로 퍼플렉서티를 평가하고, 값이 나빠졌을때만 학습률을 낮추는것(실전에서도 자주 쓰이는 기술이다, PyTorch언어모델의 구현 예를 참고했음)

실습 코드에 있는 개선된 RNN을 실행시키면 CPU에서는 2일이 걸릴수 있으니..
GPU를 사용해도 5시간 걸리고...

따라서 학습을 마친 가중치는
https://www.oreilly.co.jp/pub/9784873118369/BetterRnnlm.pkl
여기에 있다. 약 퍼플렉서티는 75정도/

## 6.6 정리

Gate가 추가된 RNN을 살펴보았다. 앞의 단순한 RNN에서는 기울기 소실이나 기울기 폭발과 같은 문제가 있었는데 그것을 게이트를 추가함으써 데이터의 기울기 흐름을 적절히 제어해 나갔다.
