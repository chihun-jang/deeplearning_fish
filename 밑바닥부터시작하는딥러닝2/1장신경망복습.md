내가 만들어낼수 없다면, 난 그것을 이해하지 못한것이다 - 리처드 파인만
2편에서는 자연어 처리와 시계열데이터 처리에 사용하는 기술에 초점

한가지 이상의 방법을 알아내기전에는 제대로 이해한 것이 아니다. - 마빈 민스키

## 수학과 파이썬 복습

### 벡터와 행렬

벡터와 행렬
벡터 : 크기와 방향을 가짐(1차원 배열로 취급할 수 있다.)
행렬 : 2차원 형태
벡터와 행렬을 확장해 N차원으로 표현한것을 tensor라 한다.
일반적으로 수학이나 딥러닝에서는 column vector의 형태를 선호한다.

> 파이썬의 넘파이 lib을 이용해서 벡터, 행렬의 생성을 손쉽게 해줄 수 있다(브로드캐스트 기능 포함)

### 벡터의 내적

```
x = (x1,x2,x3)
y = (y1,y2,y3)
x·y = x1*y1 + x2*y2 + x3*y3
```

(벡터의 내적은 두 벡터가 얼마나 같은 방향을 향하고 있는지를 나타내주기도한다)

numpy로 이러한 부분을 다룰때는 실제로 연습해보는게 제일인데
**_numpy exercises_**라는 사이트에 연습문제가 있으니 활용해보는것도 좋겠다.

## 신경망의 추론

신경망에서 수행하는 작업은 학습, 추론 두단계로 나눌 수 있다.

### 신경망 추론의 전체그림

신경망을 함수와 같이 생각해줄수도 있다.
신경망은 입력층,출력층,은닉층으로 구성이 되어있는데
edge(가중치)와 node(뉴런)의 값을 곱해서 활성화함수를 적용한 값이 그다음 계층의 입력으로 전달된다. 그리고 각 층에는 이전 뉴런의 값에 영향받지 않는 정수도 더해지는데 이 이 정수가 바로 bias(편향)값이다.

인접하는 층의 모든 뉴런과 연결되어있다는 뜻으로 **완전연결계층**이라고 한다.

그리고 이러한 전달은

```
h1 = x1*w1 + x2*w2 +b 이와 같이 다음 계층 뉴런의 input값으로 들어가게 된다.
```

이를 행렬로 나타내면 `h = xW + b`의 식으로 간소화 할 수 있다.
(이때 x는 입력, h는 은닉층의 뉴런, W는 가중치, b는 편향)(이때 형상을 일치시켜주는것이 중요하다, 형상이란 2x4행렬과 같은 것을 의미한다. 2x4 \* 4x3 )

이때 샘플데이터가 아나가 아닌 N개의 미니배치를 이용해서 학습하게 되면
x행렬이 Nx3과 같은 형태를 가지게 되고, h(은닉층의 입력)

그런데 이렇게 처리해주는 것은 완전연결계층에 의한 '선형' 변환인데
여기에 '비선형 활성화 함수'를 이용해서 신경망의 표현력을 높일 수 있다.

활성화 함수는 다양하지만 우리는 대게 시그모이드 함수나 ReLU함수를 사용하기로 한다

`σ(x) = 1/(1+exp^-x)`

이렇게 활성화 함수 처리를 해주면 비선형 변환이 된 활성화 값이 출력되게 되고 이는 다음 계층의 입력값으로 들어간다.

이러한 과정을 거쳐서 출력층으로부터 형상의 차원에 해당하는만큼 클래스를 분류할 수 있고 점를 얻을수있는데 이를 softmax함수 처리해주면 확률을 얻을 수 있다.

### 계층으로 클래스화 및 순전파 구현

완전연결계층에 의한 변환을 Affine 계층으로, sigmoid 함수에 의한 변환을 sigmoid계층으로 구현하자.
(이때 Affine이란 이름은 기하학에서의 아핀변환이 완전연결계층의 변환에 해당하기때문에 네이밍을 해주었다.)

이렇듯 계층을 클래스화 시켜놓으면 계층을 조립하여 신경망을 구성하기 수월해진다.

해당 책에서는 다음과 같은 규칙에 따라 계층을 구현한다.

- 모든 계층은 forward()와 backwrad()메서드를 가진다.
  각각 순전파와 역전파를 의미한다.
- 모든 계층은 인스턴스 변수인 params와 grads를 가진다.
  params는 가중치와 편향같은 매개변수를 담는 리스트이다.(매개변수는 여러 개가 있을 수 있어서 리스트에 보관한다.)
  grads는 params에 저장된 매개변수에 대응하여 기울기를 보관하는 리스트이다.

이처럼 params를 리스트화 시켜서 하나의 리스트로 관리를 해주게 되면 갱신이나 학습을 설명하는데 훨씬 이점이 있다.

## 신경망의 학습

학습되지 않은 신경망은 좋은추론을 할수 없다. 따라서 학습을 먼저 수행하고 학습된 매개변수를 이용해서 추론을 수행하는게 일반적이다.

### 손실 함수

신경망 학습의 척도로 손실(cost)를 사용한다.
손실은 학습시 주어진 정답값과 신경망이 예측한 결과를 비교해서 얼마나 나쁜가를 따지는 값이다.

이러한 손실은 손실함수를 사용해서 구하는데 multi class classification에서는
cost function으로 Cross entropy Error를 이용한다.
교차 엔트로피 오차는 신경망이 출력하는 클래스의 확률(이때 이 확률은 소프트맥스의 출력을 이용한다.)과 정답레이블(one-hot vector로 표현되어있다. 따라서 정답 레이블에 해당하는 출력에 대해서만 오차가 나오는거지 다른 원소들은 one-hot에 의해서 0이 되므로 계산결과에 영향을 못준다.)을 이용해 구한다.

이 책에서는 소프트 맥스를 통과시켜서 교차엔트로피오차를 계산하는 계층을
Softmax with Loss계층 하나로 구현한다(두계층을 통합하면 역전파 계산시 쉬워진다.)

### 미분과 기울기

학습의 목표는 cost를 최소화 하는 매개변수를 찾는 것인데, 이때 미분(기울기)를 사용해서 찾아나가준다.
조금 더 정확하게는 벡터(행렬)의 각 원소에 대한 미분을 정리한것이 기울기(gradient)이다.

> 수학에서는 벡터에 대한 미분만을 기울기로 한정하지만 딥러닝에서는 행렬이나 텐서의 미분도 기울기라고 불러주낟.

### 연쇄법칙

우리는 학습데이터가 주어지면 각 매개변수에 대한 손실의 gradient를 얻을수있다. 그리고 이를 통해서 매개변수를 갱신한다.
이때 손실의 gradient는 오차역전파법을 이용해서 구해준다

back-propagation의 핵심은 chain rule 인데, 이는 합성함수에 대한 미분을 생각해주면 된다.

연쇄법칙이 왜 중요하냐면 우리가 다루려는 함수가 아무리 복잡해도, 개별함수들의 미분을 이용해서 구할수 있기때문이다(국소적인 계산을 통해서 합쳐줄 수 있다.)

### 계산 그래프

계산그래프로 각 edge에 각 변수들, node에 연산을 넣어서 이어나가다 보면
우리가 구하고 싶은것은 결국 출력값에 대해 각 변수의 미분값이 궁금하고 싶은것이다.

예를 들면 `δL/δx = δL/δz * δz/δx` 이렇게 연쇄법칙으로 나타낼 수 잇는데 이때 곱셈법칙의 경우 z = x+y 에서 δz/δx를 구해보면 1이되므로
역전파를 구할때 덧셈노드에 대해서는 그냥 상류로 부터 받은 값을 하류로 흘려준다고 하는것이다.

이와같은 원리로 몇몇 연산노드에 대해서 알아보도록하자

- 곱셈노드
  z = x\*y δz/δx= y ==> 따라서 상류로 받은 기울기에 순전파시 다른 edge입력값을 곱해서 보내준다.

- 분기노드
  하나의 x edge가 두개의 edge로 분기되는 모양인데 역전파는 2개의 edge 미분값을 더해주면 된다.

* repeat 노드
  분기 노드를 일반화해서 N개로 확장한 모양이다.

* sum 노드
  repeat 노드와 반대로 복수의 노드를 0축에대해서 합하여 순전파를 진행시켜준다.

> repeat노드와 Sum노드는 서로 순전파와 역전파의 관계이다.

- MatMul 노드
  행렬의 곱셈 노드

> [...] elipsis기호를 쓰면 넘파이 배열이 가리키는 메모리위치를 고정하고 그 위치에 원소들을 덮어쓰는 것이다. 그냥 할당을 해도 되는데 이렇게 작성해주면 덮어쓰기가 되므로 깊은 복사를 하게된다. 얕은복사는 참조하는 포인터의 위치를 바꿔서 데이터만 가리켜 주고 깊은 복사는 그 데이터의 값을 지금 가리키고 있는 메모리 주소로 가져와서 데이터 자체를 복사해주는 것이다.  
> #위와같이 메모리 주소를 고정함으로써 인스턴스 변수 grads를 다루기가 더 쉬워진다.(이렇게하면 위치가 고정이니까 기울기 그룹화 작업이 한번만 이루어진다는 것이다.)

### 기울기 도출과 역전파 구현

Sigmoid 계층, Affine계층, Softmax with Loss 계층을 구현해보자

> Affine계층의 경우 MatMul계층을 이용하면 더 쉽게 구현이 되지만 이를 이용하지 않고 numpy의 method를 사용해서 구현하자

> Softmax with Loss 계층의 역전파 같은 경우 softmax계층은 역전파가 (y-t)의 형태로 깔끔하게 떨어지는데 이처럼 차이를 앞 계층에 전해주는것은 학습에 있어서 아주 중요한 성질이다.

### 가중치 갱신

1. 미니배치(훈련데이터중 무작위로 데이터를 추출한다)
2. 기울기계산 (오차역전파법으로 각 가중치 매개변수에 대해 cost function의 gradient를 구한다.)
3. 매개변수 갱신(기울기를 사용해 가중치 매개변수를 갱신)
4. 반복(1~3단계를 반복)

이때 3단계의 가중치 갱신 방법은 다양한데
그중 가장 단순한 SGD(확률적경사하강법)을 구현해보자(여기서 확률이란 미니배치에 대한 기울기의 의미다)

SGD : `W = W -η*δL/δW` (η는 학습률을 의미하며 0.01 이나 0.001값을 사용)

SGD를 사용한 매개변수 갱신은 다음과같이 해줄수있는데

```python
#의사코드
model = TwoLayerNet(...)
optimizer = SGD()

for i in ragne(10000):
    ...
    x_batch, t_batch = get_mini_batch(...) # 미니배치 획득
    loss  = model.forward(x_batch, t_batch)
    model.backward()
    optimizer.update(model.params , model.grads)
```

이처럼 최적화를 수행하는 클래스를 분리해서 구현해줌으로써 쉽게 모듈화 할수 있다.

### spiral data를 이용해 실습해보기

학습데이터를 읽어들여 신경망(model) 과 옵티마이저(최적화기)를 생성한다.
머신러닝 분야에서 문제를 풀기위해 설계한 기법(신경망ㅇ나 SVM(서포트 벡터 머신))을 가리켜 모델이라고 한다.

학습후 신경망이 영역을 어떻게 분리했는지 시각화해서 확인해보자(결정경계 decision boundary)

앞으로 이책에서는 위와 같은 학습을 많이 진행할껀데
Trainer class로 구현해서 손쉽게 재사용할수 있게 하자

```python
model = TwoLayerNet(...)
optimizer = SGD(lr = 1.0)
trainer = Trainer(model,optimizer)
```

위와 같이 작성하고`fit()`메서드를 이용해서 학습한다.
뿐만아니라 Trainer 클래스는 plot()메서드도 제공하여 손실을 그래프로 그려주기도 한다.

## 계산 고속화

신경망 고속화에 도움되는 비트정밀도와 GPU에 대해서 알아보자,

기본적으로는 float64 bit 64비트 부동소수점수를 표준으로 사용하는데
신경망 추론과 학습은 32비트로 해도 큰 차이가 없다.
32비트는 64비트의 절반이므로 메모리 관점에서는 32비트가 항상 더 좋다고 말할수 있다.

또한 신경망 계산시 data를 전송하는 버스 대역폭이 병목되는 경우가 잇는데
이럴때에서 data type이 작은게 유리하다

마지막으로 계산 속도 또한 32비트 부동소수점 수가 일반적으로 더 빠르다

```python
c = np.random.randn(3).astype('f')
c.dtype

b = np.random.randn(3).astype(np.float32)
b.dtype
```
