> 근거없이 추론하는 건 금물이야 <코난도일>

2장에서는 통계끼반기법으로 단어의 분산표현을 얻었는데
추론 기반 기법을 이용하여 분산표현을 해보자

이 추론과정에서 신경망을 사용하는데 word2vec을 사용한다.
그리고 이번 장에서는 이해하기 쉬운 단순한 word2vec을 사용해서 공부하자.

## word2vec

### 통계 기반 기법의 문제점

통계 기반 기법에서는 단어의 빈도를 기초로 단어를 표현했다.
하지만 이러한 방법은 대규모 말뭉치를 다루게 될때 문제가 발생하는데, 말뭉치의 어휘수^2만큼의 행렬 사이즈가 생성이 되게 되고 이런 행렬에 SVD를 적용하는 것은 쉬운 일이 아니다.
(실제로 처리하는 속도는 O(n^3))

통계끼반 기법은 전체의 통계를 이용해 1회의 처리만에 단어의 분산표현을 얻는 한편, 추론 기반 기법에서는 미니배치로 학습한다.
(반복적으로 학습하며 가중치를 갱신한다.)

이렇게 크기를 나누어 처리하게 되면 GPU를 이용한 병렬 계산도 가능해져서 학습 속도 향상도 생각해 볼수 있다.

### 추론 기반 기법 개요

`you ? goodbye and i say hello` 에서 ?에 들어갈 단어를 추측하는 작업이 추론 기반 기법이다.
물론 추론 기반 기법도 통계기반기법처럼 분포가설에 기초한다.
(분포가설은 단어의 의미는 주변 단어에 의해 형성된다는 가설)

신경망을 학습시켜서 해당 자리에 단어들의 출현 확률을 추측할 수 있게 하는 것

### 신경망에서의 단어처리

신경망에서는 단어를 있는 그대로 처리할 수 없으니 고정 길이의 벡터로 변환해야한다.( 원핫 인코딩을 통해서)
따라서 단어->벡터->계층의 처리가 가능해졌다는 말이다.
그러면 단어의 갯수만큼 입력층의 뉴런이 생성이되고 이것을 은닉층과 완전연결계층으로 표현하면 행렬 곱 계산에 해당한다(편향 X)

## 단순한 word2vec

모델을 신경망으로 구축해보자. (이번 절에서는 word2vec에서 제안하는 CBOW(continuous bag-of-word)모델 사용)

> 원래 word2vec은 프로그램이나 도구였는데 요즘에는 신경망 모델을 가리키는 경우도 더러 있다.

### CBOW 모델의 추론처리

CBOW 모델은 맥락으로 부터 타깃을 추측하는 용도의 신경망이다.
(타깃은 중앙단어, 그 주변 단어들이 맥락)
CBOW 모델의 입력은 맥락이다. 맥락은 단어들의 목록인데 일단 one-hot으로 변환하여 CBOW모델이 처리할 수 있도록 준비하자.

입력하는 단어가 2개이면 입력층도 2개가 되는데 은닉층은 완전연결계층에 의해 입력층의 입력의 평균값이 들어가게 되고 출력층은 이러한 은닉층을 지나 다시한번 완전연결계층을 지나 각각의 단어의 점수(softmax를 통과했으면 확률)로 표시된다.

이때 W에는 각 단어의 분산표현이 담겨있다고 볼수 있는데, 학습을 진행할수록 이 분산표현들이 갱신될 것이다.

> 은닉층을 입력층의 뉴련보다 적게 하는것이 핵심이다 그래야지 단어예측에 필요한 정보를 간결하게 담을수 있고, 밀집벡터 표현을 얻을수 있따.

### CBOW 모델의 학습

올바른 예측을 할 수 있게 가중치를 조정하는 일을 한다.(정확하게는 W_in과 W_out에 모두 출현패턴을 파악한 벡터가 학습된다.)
(이때 분산표현은 모델 학습시에 사용한 말뭉치의 종류에 따라서 달라질수 있다.)

우리가 다루는 모델은 다중 클래스 분류를 수행하는 신경망이므로 softmax와 cross entropy 만이용하면 된다.(우리는 softmax with loss로 구현한다.)

### word2vec의 가중치와 분산표현

- W_in측의 가중치만 이용한다
- W_out측의 가중치만 이용한다
- 양쪽 가중치 모두 이용한다

word2vec에서는 첫번째 안인 W_in만 이용한다가 가장 대중적인 선택이다(word2vec과 비슷한 GloVe에서는 두 가중치를 더했을때 더 좋은 결과가 나왔다.)

## 학습 데이터

### 맥락과 타깃

word2vec에서 이용하는 입력은 맥락이고 이는 문장에서 앞 뒤 단어(온점 포함)을 제외한 단어들로
맥락과 타깃을 구분하여 input과 정답 레이블로 사용할 수 있다.
(타깃은 1개이지만 맥락은 여러개가 될 수 잇으므로 contexts로 작성해준다)

corpus를 주면 맥락과 다깃을 만들 함수를 구현할건데, 이때 맥락의 0번째 차원은 각 맥락 데이터가 저장된다(contexts[0] 에는 0번째 맥락이 저장)

### 학습코드 구현

실습 코드 참고

## word2vec

CBOW모델을 확률 표기법으로 기술해보면 맬락 w_i-1 과 w_i+1이 주어졌을때 타깃 w_i가 될 확률의 수식ㅇ느
`P(W_i|W_i-1,W_i+1)` 이를 교차 엔트로피 오차와 적용해보면
tk의 정답레이블 표현이 원핫 벡터로 표현 됨에 의해

`L = -logP(W_i|W_i-1,W_i+1)`가 된다. 이를 음의 로그 기능도, (negative log likelihood)라 한다.
이는 샘플 데이터 하나에 대한 손실함수고 말뭉치 전체로 확장하면

T개 만큼 더해서 평균을 내주면 된다.

### skip-gram 모델

word2vec 모델은 CBOW모델과 skip-gram모델을 제시한다.
skip-gram은 맥락과 타깃을 역전 시킨 모델이다. 따라서 입력층이 하나이고 출력층은 맥락의 수만큼 존재하게 된다. 이 출력층은 각각 개별적으로 손실을 구하고 (개별손실을 모두 더해 최종 손실로한다)

`P(W_i-1, W_i+1| W_i)`의 모델링이다. skip-gram모델에서는 맥락 단어들 사이에 관련이 없다고 가정(조건부 독립이라고 가정) `P(W_i-1, W_i+1| W_i)` = `P(w_i-1|W_i)P(W_i+1|W_i)` 으로 분해해 줄수 있다.

그리고 손실함수를 유도하면 -(logP(w_i-1|W_i) + logP(W_i+1|W_i))가 된다.(하나의 샘플데이터 손실함수)
전체는 T만큼 더하고 T만큼 나누어 평균을 구해준다.

CBOW 모델보다 skip-gram모델이 분산표현의 정밀도 면에서 더 좋은 경우가 많다
(특히 말뭉치가 커질수록 저빈도 단어 유추문제의 성능면에서 더 뛰어나다.)
그치만 속도는 CBOW모델이 더 빠르다.(skip-gram은 손실을 맥락 수만큼 구해야하니까)

### 통계기반 vs 추론기반

어휘에 추가 단어가 생기면 통계기반은 처음부터 학습해야 하는 반면 추론기반은 이미 학습한 가중치를 초깃값으로 사용해 다시 학습을 진행한다.

- 분산표현의 성격

  - 통계기반 : 단어의 유사성이 인코딩 된다.
  - word2vec : 단어의 유사성 + 단어사이의 패턴까지도 파악되며 인코딩 된다.

  > 하지만 단어유사성을 정량평가해보면 두 기법의 우열을 가릴수가 없었다한다.
  > 무엇보다도 두 기법은 특정조건 하에 서로 관련이 되어있다.
  > (그리고 word2vec이후 통계기반과 융합한 Glove 기법이 등장했다.)
