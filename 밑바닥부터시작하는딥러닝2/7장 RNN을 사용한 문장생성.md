> 세상에 완벽한 문장은 없어 완벽한 절망이 없듯이 - 무라카미 하루키[바람의 노래를 들어라]

LSTM을 이용해서 재미있는 App을 구현해보자

언어 모델을 사용해서 문장생성을 해보자. 그리고 더 자연스러운 문장을 생성하는 것도 연습해보자.

그리고 seq2seq라는 새로운 신경망도 다뤄보자.(시계열에서 시계열을 뜻함)
즉 RNN두개를 연결하는 간단한 방법으로 seq2seq를 구현.

seq2seq는 기계번역, 챗봇, 메일등 다양하게 응용 가능하다.

## 7.1 언어모델을 사용한 문장생성

### 7.1.1 RNN을 사용한 문장생성의 순서

언어모델에게 문장을 생성시키는 순서를 설명해보자
항상 예는 "you say goodbye and i say hello"

만약 위의 말뭉치로 학습한 언어모델은 I를 입력했을때 say를 가장 높은 확률분포로 보여줄 것이다.

- 확률이 가장 높은 단어를 선택하는 것은 결정적인 방법이고
- 확률에 따라서 확률적으로 선택하는 방법은 선택되는 단어(샘플링 단어)가 매번 다를 수 있다.

위에서 say를 선택했으면 다음으로 say를 입력해서 샘플링을 다시금 실행한다.
이러한 작업을 원하는 만큼 반복하거나 eos 같은 종결기호가 나타날때까지 실행하면 새로운 문장이 생성된다.

따라서 새롭게 생성한 문장은 훈련데이터에서 사용된 data의 정렬패턴을 학습한 것이므로, 새로운 문장을 생성하게 된다.
따라서 정렬, 출현패턴을 올바르게 학습하면 새로이 생성되는 문장도 자연스러울 것이다.

### 7.1.2 문장 생성 구현

문장을 생성하는 method에는 skip_ids 라는 list도 잇는데, PTB datasest는 문장을 전처리해둔것으로
희소한단어 unk, 숫자는 N으로 처리가 되어있다. 그리고 문장을 구분하는데는 eos라고 처리가 되어있다.

predict() 를 통해서 각 단어의 점수를 출력하고,
softmax함수를 통해서 정규화를 진행한다. 이를 통해서 확률분포 p를 얻을 수 있다.
확률분포 샘플링은 np.random.choice()를 사용한다.

### 7.1.3 더 좋은 문장 생성

좋은 언어모델에 더 큰 말뭉치를 사용하면 더 자연스러운 문장을 생성해 줄 것이다.

## 7.2 seq2seq

세상에는 언어, 음성, 동영상과 같은 시계열 data가 넘쳐난다.
그 외에도 기계번역이나 음성인식처럼 시계열 데이터의 변환도 생각할 수 있다.
이러한 변환의 문제로는 2개의 RNN을 이용해 seq2seq방법을 이용해서 해보도록 하자

### 7.2.1 seq2seq의 원리

seq2seq를 Encoder-Decoder 모델이라고도 한다.

- Encoder(input data를 부호화)
- decoder(복호화, 암호 -> 문자 느낌)

seq2seq에서는 enco와 deco가 협력해서 시계열 data를 다른 시계열 data로 변환한다.

시계열 data가 들어와서 은닉상태 벡터로 변환되면 은닉상태 h에 번역하는데 필요한 정보가 encoding되어있고, 은닉상태 h는 고정길이 vector이다.
즉 encoding은 임의의 길이의 문장을 고정길이벡터로 변환하는 작업

앞에서 다룬 문장생성모델, 신경망과 같은 구성으로 decoder가 만들어지는데 LSTM계층은 벡터 h를 받는게 다른점이다.

즉 전체적으로 Encoder LSTM과 Decoder LSTM이 연결되게 되고 은닉상태 h가 둘 사이를 이어주게 된다.

### 7.2.2 시계열 데이터 변환용 장난감 문제(toy problem)

시계열 변환문제의 예로 +를 다뤄보자
"57+5" => "62" 이렇게 변환하는것을 학습시켜보자

지금까지는 단어단위로 분할했지만 꼭 단어단위로 해야하는것은 아니다.

더하는 숫자의 크기가 다르면 그 샘플의 데이터 시간 크기가 다르므로 가변길이 시계열 데이터를 다루는게 되는데, 신경망 학습시 미니배치처리를 하기위해서는 무언가 추가적인 작업을 해줘야함.(미니배치는 한번에 다수의 샘플을 처리하므로, 미니배치의 샘플들은 데이터 형상이 모두 같아야한다.)

이때 가변 길이 시계열 data를 미니배치로 학습하기 위한 방법으로는 padding을 사용하는 것이다. (padding이란 의미없는 데이터를 채워 모든 data의 길이를 균일하게 맞추는 것)

eg. 0~999를 더하는 문제를 생각해보면 입력 data의 최대길이는 7자이고
출력의 최대길이또한 4이겠지만 출력문자를 알려주기위해 앞에 *를 추가하고, 이 *를 문자열 생성을 알리는 신호로 사용하자

이렇게 padding을 입력하면 가변길이 data도 처리해줄수있지만 정확성이 중요하다면 seq2seq에 패딩 전용 처리를 추가해주면 된다(decoder에 입력된 ㅇata가 패딩이면 loss에 반영되지않게, soft 계층에 mask를 추가하든지 아니면 enc계층에 padding이 입력되면 그대로 출력을 하던지)

## 7.3 seq2seq 구현

LSTM계층을 이용해서 encoder를 구성하는데
그전에 Embedding계층을 거쳐서 가게된다.

embedding계층에는 문자ID를 문자 벡터로 변환하게 된다.
LSTM계층은 시간방향으로 h와 cell을 출력하고 위로는 h만 출력한다.

Encoder클래스는 `__init__()`,`forward()`, `backward()`메서드로 구성이 되어있는데 어휘수(문자종류), 문자벡터의 차원수, LSTM계층의 은닉상태벡터의 차원수 등을 받는다.

encoder의 순전파에서는 Time LSTM의 마지막 시각의 h만을 encoder의 forward()의 method의 출력으로 반환한ㄷ.

역전파는 LSTM계층의 h에 대한 기울기가 dh인수로 전해지고, 이는 decoder가 전해주는 기울기 이다.
원소가 모두 0인 tensor dhs를 생성하고 dh를 dhs의 해당 위치에 할당. 그다음 backwrad()메서드를 호출할 뿐이다.

### 7.3.2 Decoder 클래스

Encoder클래스가 출력한 h를 받아 목적이 되는 다른 문자열을 출력하는 클래스이다.

RNN으로 문장을 생성할때는 train시와 predictt의 데이터 부여방법이 다른데, 학습시에는 정답을 모두 알고있기에 시계열 data를 한번에 줄수있지만 predict시에는 최초 시작 문자를 하나주고 순서대로 하나씩 샘플링을 하여 다음 문자를 생성해 나간다.

그리고 차이점은 샘플링을 할때 확률적인 방법보다 결정적인 방법을 통해서 결정해주자.

affine 계층이 출력하는 점수를 argmax라는 노드를 사용해서 최댓값을 선택해주자(실제로 predict헤즐떼)

그리고 이번문제에서는 Encoder의 출력 h를 Decoder의 Time LSTM계층의 상태로 설정해주고, Time LSTM계층은 상태를 갖도록 해준다.

### 7.3.3 seq2seq2클래스 구현

Encoder Class와 Decoder Class를 연결하고 Time Softmax with Loss계층을 이용해 손실을 계산하는것.

### 7.3.4 seq2seq2평가

seq2seq2의 학습은 기본적으로 신경망의 학습과 같은 흐름,

1. 학습데이터에서 미니배치 선택
2. 미니배치로부터 기울기 계산
3. 기울기를 사용해 매개변수 갱신

## 7.4 seq2esq 개선

seq2를 세분화 해서 속도를 개선해보자

### 7.4.1 Reverse(입력데이터 반전)

57 + 5 ==> 5 + 75 와 같이 반전시키는것

배열의 행을 반전시키려면 x_train[:,::-1]이라는 표기법을 사용하면 된다.
직관적으로는 반전시키게 되면 기울기 전파가 원할하게 이루어지기 때문이다.
즉 나는 고양이 I am a cat에서 나 와 I의 사이는 는 고양이만큼 떨어져있는데 이를 반전시키면 나와 I가 그만큼 가까이 있으므로 학습효율이 좋아진다고 생각할수 있다.

### 7.4.2 엿보기(Peeky)

Encoder가 출력하는 h를 전해받는 LSTM계층은 첫 LSTM계층만이 받게된다.
따라서 이 h를 더 이용할수 있으면 좋지 않을까 하는 차원에서 생각된 기법으로 h를 다른 계층(affine과 LSTM계층)에도 전해주는 것이다.

이때 affine 계층과 LSTM계층은 h가 입력되는것도 있고, x가 입력되는 것도 있는만큼 두개의 arg가 들어오게 되므로 concat노드를 거쳐 1개로 입력이 들어오게 된다.

forward를 할때 h를 np.repeat()로 시계열 만큼 복제해서 저장해놓고 np.concatenate()를 이용해서 hs와 embedding계층의 출력을 연결하는 작업을 한다.

> 따라서 reverse와 Peeky를 사용해서 아주 획기적으로 개선시킬수 있다.
> 단! peeky를 사용하면 가중치 매개젼수가 생겨 계산량이 늘어나는 핸디캡이 있다.

## 7.5 seq2seq를 이용하는 Application

- 기계번역 : 한 언어의 문장을 다른 언어의 문장으로 변환
- 자동 요약 : 긴문장을 짧은 문장으로 변환
- 질의 응답 : 질문을 응답으로 변환
- 메일 자동응답 : 받은 메일의 묹아을 답변글로 변환

즉 seq2seq2는 2개가 짝을 이루는 시계열 데이터를 다루는 문제에 이용가능

### 7.5.1 챗봇

상대의 말과 자신의 말로 구성되기에 seq2seq2로 학습시킬수 있다.

### 7.5.2 알고리즘 학습

코드로 짜여진 알고리즘도 결국 시계열 데이터 이므로 학습시킬수 있을 것이다.

> 다음장에서 NTM모델을 이용해서 알고리즘을 재현해보자

### 7.5.3 이미지 캡셔닝

seq2seq2는 텍스트 뿐만아니라 img나 음성도 처리할수 있다.
이미지 -> 문장으로 변환하는기술인데

Encoder가 LSTM에서 CNN(합성곱 신경망, Conveolutional Neural Network)로 바뀐 것,
이때 CNN의 최종 출력은 특징 map이다.(3차원으로 구성됨)
따라서 CNN의 출력을 flattening(평탄화) 한다음 완전연결인 Affine에서 변환한다.

> 이때 CNN에서 사용하는 신경망은 입증된 신경망, 그리고 가중치로는 다른 이미지 데이터셋으로 학습을 끝낸 것을 이용한다

> 이미지 캡셔닝의 예는 im2txt가 있따.

> 이는 설명을 담은 학습 데이터와 이 학습데이터를 효율적으로 학습하는 seq2seq가 있기때문에 가능했다.

## 7.6 정리

이처럼 RNN을 활용한 seq2seq는 많은 어플리케이션에 적용할수도 있고 가능성을 지니고 있다.

다음장에는 어텐션이라고 하는 seq2seq2를 개선하는 아이디어를 배우는데 이는 딥러닝에서 가장 중요한 기법중 하나이다.
