# chapter 4. word2vec 속도 개선

> 모든 것을 알려고 애쓰지 마라 그러다보면 아무것도 기억 할수 없다. - 데모크리토스

3장에서는 word2vec의 구조를 배우고 CBOW 모델을 구현해봤다.
CBOW는 2층 신경망이라 간단하게 구현할 수 있지만,
말뭉치에 포함된 어휘가 많아지면 계산량도 많아진다는 문제점이 있다.

따라서 이번장에서는 word2vec의 두가지 사안을 개선해보도록 하자

1. Embedding이라는 새로운 계층 도입
2. Nagative Sampling이라는 새로운 cost function 도입

==> 진짜 word2vec완성

## 4.1 word2vec 개선

기존의 CBOW모델은 어휘가 100만개 일때 병목 문제가 발생할 수 있는데,

- 입력층의 원핫 표현과 가중치 행렬 W_in의 곱계산
  원핫 표현으로 바꾸게 될때 vector의 크기도 커지게 된다.
  그리고 이 vecotr의 처리에도 상당한 ㄹ소스를 사용하게 되는데 embedding계층을 도입하는 것으로 해결해보자.

* 은닉층과 가중치 행렬 W_out의 곱 및 softmax계층의 계산
  은닉층과 W_out의 곱만해도 계산량이 상당하다. 그리고 softmax에서도 다루는 어휘가 많아 계산량이 증가하는데 이는 nagative sampling이라는 새로운 cost function을 도입해서 해결한다.

### 4.1.1 Embedding 계층

: Embedding 계층에 단어 임베딩(분산표현)을 저장하는 것

> 자연어처리분야에서 밀집벡터 표현을 단어 임베딩 혹은 단어의 분사표현이라 한다.

### 4.1.2 Embedding 계층 구현

행렬에서 특정 행을 추출하는 것은 W[2]처럼 간단하게 명시해줄수 있다.(common/layers.py 의 Embedding class를 참고하자)

MatMul계층을 Embedding계층으로 전환하면 MEm 사용량도 줄이고 쓸데없는 계산도 생략할수 있다.

## 4.2 word2vec 개선 2

은닉층 이후의 처리를 해결해보자(행렬 곱과 softmax 계층의 계산)
softmax 대신 negative sampling 이라는 기법을 사용하면 어휘가 아무리 많아져도 계산량을 낮은 수준에서 일정하게 억제할 수 있다.

### 4.2.1 은닉층 이후 계산의 문제점

은닉층 이후 계산의 문제점을 알아보기 위해 어휘를 100만개로 극단적으로 한번 늘려보자,

- 은닉층의 뉴런과 가중치 행렬(W_out)의 곱
  거대한 행렬을 곱하는 문제로, 시간과 메모리 소요가 많다. 따라서 행렬 곱을 가볍게 만드는게 중요하다.

- Softmax 계층의 계산
  위와 같이 연산이 오래걸리게 된다.

### 4.2.2 다중 분류에서 이진 분류로

네거티브 샘플링 기법의 핵심 아이디어는 이진 분류에 있다.
즉, 다중 분류를 이진 분류로 근사하는 것이 네거티브 샘플링을 이해하는 중요 point이다.

아이디어는 기존에 타깃 부분에 무슨 단어가 들어갈지 묻는 경우에서(전체 단어에 대해서 질문을 던짐)
특정 단어로 한정지어, 해당 단어가 맞는지 아닌지 Yes or No로 대답할 수 있게 하는것이다.

그리고 이렇게 특정 단어의 점수를 가져오고, 시그모이드 함수를 이용해서 확률로 변환한다.
손실함수로는 교차엔트로피 오차를 사용한다.
(소실함수가 같은만큼 Softmax with Loss 계층의 코드를 조금만 손보면 Sigmoid with Loss 함수가 된다, 출력층의 뉴런을 2개만 사용할 경우)

이때 시그모이드 함수 + 교차 엔트로피 오차의 역전파로는 y-t라는 깔끔한 값이 가는데 y값이 t랑 가까울수록 오차가 작게 전달이 된다는 의미도 해석을 해보자

### 4.2.5 네거티브 샘플링

위의 다중분류를 이진분류로 바꾸는 과정도 긍정적인 답변에 대한 케이스이다.
만약 네거티브상황에 대해서 학습을 시킬거라고 모든 경우에 대한 학습을 진행하게 되면 애당초 우리가 생각했던 size를 줄이는게 아니기때문에
근사적으로 부정적인 예를 몇가지 샘플링 해서 그 예에 대해서 학습을 진행한다.

==> 긍정적 예의 손실 + 샘플링된 몇개의 부정적 예의 손실 = 최종손실

### 4.2.6 샘플링 기법

무작위 샘플링 보다 말뭉치의 통계를 기반으로 샘플링(5~10개)하자.
즉, 단어출현횟수를 구해 확률분포로 나타내고 이를 기반으로 샘플링 하면 된다.
(많이 등장하는 단어를 샘플링한다)

word2vec의 네거티브 샘플링에서 0.75제곱을 해주는데 이는 확률이 낮은 단어의 확률을 살짝 높일 수 있다는 장점이 있다.

> Unigram은 하나의 단어를 뜻하고, 한 단어를 대상으로 확률 분포를 만든다는 의미가 녹아있다.

### 4.3.3 CBOW모델 학습

일반적으로 윈도우크기는 2~10개 은닉층 뉴런수는 50~500개정도면 좋은 결과가 나올 수 있다.

파이썬의 피클기능을 이용해 파이썬 코드의 객체를 파일로 저장하자.

word2vec으로 얻은 단어의 분산표현은 비슷한 단어를 가까이 모을뿐 아니라 더 복잡한 패턴을 파악한다.
"man: woman = king : ? " 과 같은 문제를 풀수있게 되는 것

이 책에서는 analogy라는 함수를 통해서 이와같은 유사성을 찾아낼수 있다.
더 큰 dataset으로 학습을 시키면 보다 더 정확한 결과를 받아 들수 있을 것이다.

## 4.4 word2vec 남은 주제

단어 분산표현의 장점은 비슷한 단어를 찾는것만이 아니라 전이학습이 가능하다는 것인데, 자연어 문제를 풀때 word2vec을 처음부터 학습하는 일은 없다.
먼저 큰 말뭉치로 학습을 끝낸 후, 해당 분산표현을 각자의 작업에 이용하는 것이다.

또한 단어의 분산 표현은 단어를 고정길이 벡터로 변환해주는 장점도 있는데,
문장도 고정길이 벡터로 변환할수 있다.(단어의 분산표현을 구하고 합하는 방법, bag-of-words) 또한 앞에서 설명할 순환신경망 RNN을 사용하면 word2vec의 분산표현을 이용하면서 문장을 고정 길이 vec으로 변환할수 있다.

벡터로 표현하는게 왜 중요하냐면 일반적인 ML기법에 사용할수 있기때문이다.

eg. 메일 자동분류 : 데이터(메일)을 수집하고, 사용자가 보낸 메일에 수동으로 label을 붙인다. 그리고 레이블링 작업이 끝나면 학습된 word2vec을 이용해 메일을 vec으로 변환하고, 어떤 분석시스템에 메일과 label을 입력해 학습을 진행한다.

따라서 단어의 분산표현의 방법으로 벡터화를 할수 있는 것은 아주 중요하다.

> 단어의 분산표현의 우수성은 어떻게??
> 분산표현과 분류를 분리해서 학습할수 있는데, 따라서 분산표현의 우수성을 분리해 평가하는 척도로 단어의 유사성과 유추문제를 활용한 평가가있다.
> ==> 말뭉치는 크고, 단어벡터 차원수는 적당한 크기가 좋다.

### 정리

계산을 단순화 하는 핵심은 모두 대신 일부를 처리하는 것이다.
