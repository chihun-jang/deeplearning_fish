> 해당 장에서는 텍스트를 단어로 분할하는 처리나 단어를 단어 ID로 변환하는 처리등을 구현한다.

자연어란?
우리가 평소에 쓰는 말을 자연어라한다.
NLP는 말그래도 Natural Language Processing 이고
컴퓨터에게 우리의 말을 이해시키기 위한 기술이다.

단어의 의미이해시키기.(단어는 의미의 최소단위)

# 시소러스(Thesaurus, 유의어사전)

뜻이 같은 단어나 뜻이 비슷한 단어가 한 그룹으로 분류되어있다.
자연어 처리에 사용되는 시소러스는 단어사이의 상,하위까지 관계를 정의해둔 경우가 있다.
그러면 컴퓨터는 단어의 의미를 간접적으로라도 이해하는 범위가 되었다.

## WordNet

WordNet(NLTK)을 사용하면 유의어나 단어 네트워크를 이용할 수 있다.
그리고 단어간의 유사도를 구할수도 잇는데, 이에 대한 실습은
부록을 참고하자

> 하지만! 이렇게 살마이 라벨링 하는 방식에는 문제점이 존재하는데

- 시대 변화에 대응하기 힘들다는 것이다.(시대에따라 단어의 의미는 변하고 그러면 사람이 손수 수정작업을 해줘야한다,)
- 사람을 통해 라벨링을 하기때문에 리소스가 너무크다
- 단어의 미묘한차이를 표현할수 없다.(단어가 주는 뉘앙스까지 상세히 표현하기 어렵다.)

이러한 문제를 해결하기위해 통계 기법 및 추론기반을 사용하게 되었고 사람의 노동력이 현저히 줄어들고있다.

## 통계 기반 기법

이제부터 통계 기반 기법을 살펴보면서 우리는 말뭉치를 이용할 것이다. 말뭉치랑 대량의 텍스트 데이터로 맹목적으로 수집된게 아닌 자연어 처리 연구 및 애플리케이션을 염두하고 수집된 텍스트 데이터를 말뭉치라 한다.

결국 말뭉치란 텍스트데이터에 지나지않지만, 그 안의 문장들은 사람이 쓴만큼, 자연어에 대한 사람의 지식이 충분히 담겨있다고 볼 수 있다.(문장을 쓰는 방법, 단어 선택방법, 단어의 의미)

통계기반은 이러한 말뭉치에서 자동으로, 효율적으로 핵심을 추출하는 것이다.(일반적으로 말뭉치는 품사와 같은것도 같이 제공되어 컴퓨터가 다루기 쉬운 형태로 가공되어 주어진다.)

일단은 하나의 문장을 전처리(단어로 분할, 단어 ID목록 변환)하는것부터 시작하자.

### 단어의 분산표현

색깔에 이름을 부여해서 표현하는것보다 RGB를 이용하면 보다 적은 리소스로 많은 색의 표현이 가능해지고,
관련성 및 정량화 하기도 쉬워진다.

이처럼 단어도 벡터표현을 하게 되는데 NLP에서는 분산표현(distributional represenntation)이라고 한다.

> 단어를 고정길이의 밀집베거로 표현, (대부분의 원소가 0이 아닌 실수인 벡터)
> 예를 들면 [0.21, -0.45, 0.83]과 같은 모양이다.

### 분포 가설

단어의 의미는 주변 단어에 의해 형성된다.
즉 그 단어 자체에는 의미가 없고 단어가 사용된 맥락이 의미를 형성한다는 것이다.
(비슷한 의미의 단어는 비슷한 맥락에서 자주 등장)

NLP에서의 맥락은 특정 단어를 중심으로 그 주변단어를 말하고,
맥락의 크기(주변단어의 범위)를 window size 라고 한다.(좌,우가 다를수도 있다.)

이때 윈도우 사이즈 내에 존재하는 단어를 one-hot 처럼 1로 표현해주면 특정 문장안에서 단어가 가지는 맥락이 표로 나타내진다.
co-occurrence-matrix(동시발생행렬)

동시발생행렬을 활용하면 단어를 벡터로 나타낼수 있다. (word가 id값으로 변환이되고 그때 동시발생행렬의 index값을 찾아가서 해당 단어와 인접한 단어가 무엇인지 체크해주는것이다.)

물론 말뭉치로부터 동시발생행렬을 자동으로 만들어줄 수도 있다.

### 벡터간 유사도

벡터간의 유사도를 측정하는 방법으로는 내적이나 유클리드 거리등이 있다.
하지만 그중에서 우리는 코사인 유사도를 이용할 것이다.
이의 핵심은 벡터를 정규화 하고 내적을 구하는 것이다.

이를 좀더 쉽게 설명하면 가리키는 방향이 얼마나 비슷한가를 따지고,
방향이 완전히 같다면 코사인 유사도가 1 완전히 반대면 -1이 된다.

벡터간의 유사도를 이용해서 유사한 단어의 상위 정렬을 할때
argsort()메서드를 사용할수 있는데 argsort()메서드는 numpy배열의 원소를 오름차순으로 정렬한다.(단 반환값은 배열의 index이다)

이렇듯 기본적인 통계기반 기법을 이용해서도 단어간의 유사도를 계산해 낼수 있다.

## 통계 기반 기법 개선

### 상호 정보량

동시발생행렬의 발생 횟수만을 특징으로 삼는것은 그렇게 좋은 방법이 아니다.
예를 들어서 the car처럼 묶여서 많이쓰이는데 이떄 car가 drive라는 단어보다 the라는 단어와 더 관련성이 강하다고 나올것이기때문에
빈도수에 따라서 평가하는 부분의 보완이 필요하다.

점별 상호정보량(Pointwise Mutual Information(PMI))
PMI를 사용하면 단어가 단독으로 출현하는 횟수를 고려해서 cnt 해주기때문에 단어간의 관계를 보다 잘 나타낼수 있다.

> PMI에도 문제가 잇는데 두 단어의 동시발생 횟수가 0이면 식에의해서 음의 뭏ㄴ대로 가게되는데 이 문제를 피하기 위해 실제로는 PPMI(양의 상호정보량을 사용)
> PPI(x,y) = max(0, PMI(x,y))

우리는 0으로 나누는 것을 방지하기위해서 eps(입실론)을 이용해서 아주 작은 값을 더해준다.

PPMI행렬을 사용해도 말뭉치으 어휘수가 증가하면 차원수가 증가한다는 문제가 있는데, 벡터의 차원도 똑같이 증가하는게 문제다.
뿐만아니라 대부분의 벡터의 원소는 0으로 중요하지 않다는 뜻인데, 이런 벡터는 노이즈에 약하고 견고하지 못하다는 약점도 있다.
(원소 대부분이 0인 행렬 또는 벡터를 희소행렬, 희소벡터라고 한다. (sparse matrix))

### 차원 감소(dimensionality reduction)

벡터의 차원을 줄이는 방법을 말한다.(중요한 정보는 유지하면서)
2차원 좌표에 표시된 데이터들을 새로운 축을 도입해서 하나의 축으로만 표현할수 있따( 데이터가 넓게 분포되도록 고려해야한다.)
1차원값만으로도 데이터의 본질적인 차이를 구별할 수 있어야 한다.

차원감소의 핵심은 sparse vertor에서 중요한 축을 찾아내어 더 적은 차원으로 다시 표현하는것이고 이러한 과정을 거치면 희소벡터는 밀집벡터(대부분이 0이 아님)으로 변환된다.

차원을 감소시키는 방법은 여러가지가 있는데, 특잇값 분해(SVD, singular value Decomposition)을 이용한다.
SVD는 임의의 행렬을 세 행렬의 곱으로 분해한다.

### SVD에 의한 차원 감소

SVD는 numpy의 linalg(선형대수)모듈이 제공하는 svd메서드로 실행할 수 있다.

SVD에 의해 차원감소를 시켜주고 2차원상에서 그래프를 그려주면 시각적으로 유사도를 파악해볼수 있다.
그런데 말뭉치가 적으면 정확도가 다소 떨어지게 나올수 있으므로 PTB dataset을 이용해서 똑같은 실험을 수행해보자.

> 행렬의 크기가 N이 되면 SVD의 계산은 O(N^3)으로 나오게 되는데 이는 감당하기 어려운 수준이므로 Truncated SVD를 이용해서 특잇값이 작은것은 버리는 방식으로 성능향상을 꾀해준다. 다음 절에서도 옵션으로 사이킷 런 lib의 Truncated SVD를 이용한다.

### PTB dataset

PTB(펜 트리뱅크)는 주어진 기법의 품질을 측정하는 벤치마크로 자주 이용된다.
한 문장이 하나의 줄로 저장이 되어있는데, 우리는 각 문장들을 모두 통틀어서 하나의 큰 시계열 데이터로 취급해줄것이다.(문장별 단어의 빈도를 계산할수도있다)

PTB dataset 을 이용해서 사이킷런의 고속 SVD를 처리해주면 우리가 보는 직관과 유사한 단어들간의 연관도가 나타나게된다(더 좋은 단어 벡터를 얻어냈다)
이것이 분산표현이고 각 단어는 고정길이의 밀집벡터로 표현되어있다.

## 정리

자연어를 컴퓨터에 단어의 의미를 이해시키기라는 주제로 설명했다.
