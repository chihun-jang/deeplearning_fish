### 딥러닝

딥러닝은 층을 깊게 한 심층 신경망이다.

#### 더 깊게

- 활성화 함수를 ReLU로 사용
- 완전연결 계층 뒤에 Dropout계층사용
- Adam을 사용해 최적화
- 가중치 초깃값은 "He의 초깃값"

이를 이용하면 인식률을 높일수 있는데 이를통해 우리 사람들의 인식률과 비슷한 정도로 인식률을 개선시킬수있다.

What is the class of this image? 사이트를 보면 다양한 dataset을 대상으로 그동안 사용하는 기법들의 정확도를 정리해두었다.

MNIST 문제의 경우에는 일반적 문제에 비해서 비교적 단순해서 층을 굳이 깊게 안해도 되지만 일반 사물인식에서는 층을 깊게하면 정확도 개선에 효과가 있다.

> 정확도 향상에 도움이 되는것들 : 앙상블학습, 학습률감소, 데이터확장

- 데이터확장 : train img 를 인위적으로 확장한다. 회전을 하거나 이동을하거나. data가 별로 없을때 효과적이다.(crop, flip 포함)

층을 깊게하는게 중요한 이유:
사실 이론적으로 근거가 탄탄한 것은 아니다.
하지만 층을 깊게한 결과로 정확도가 좋아지는데, 이점으로는
적은 매개변수로 같거나 그 이상의 표현력을 달성할수있다.

ex) 5X5 합서옵 연산 1회는 3X3연산 2회를 수행해서 대체할수 있다.
매개변수도 25개와 18개로 갯수가 적어진다.

> 작은 필터를 겹쳐서 신경망을 깊게할때 장점은 매개변수 수를 줄여 넓은 수용영역을 소화할수 있다는데 잇다.(수용영역이란 뉴런의 변화를 일으키는 공간영역), 층을 거듭하면서 ReLU등 활성화 함수를 합성곱 계층 사이에 끼워넣어서 신경함수의 표현력이 개선된다.(활성화 함수가 비선형이고, 이를 통해서 더 복잡한 것도 표현할수 있기 때문)

학습의 효율성도 층을 깊게하는게 이점이다. 층을 깊게해서 train data양을 줄여
고속으로 수행할수있는데
앞장에서 보면 층이 깊어질수록 사물에 대한 이해가 깊어졌던만큼
개를 분류하는 문제가 있다고 할때 얕은 신경망의 경우에는 개의 특징을 한번에 많은것을 이해해야하므로 data도 많이 필요하고 학습시간도 오래걸리게 된다.

따라서 계층적으로 해주게 되면 각 계층마다 특정 단순문제에 더 집중할수있어서 좋다.(이 모두는 빅데이터와 GPU로 인해 가능해졌다.)

딥러닝이 주목받기 시작한것은 2012년 ILSVRC에서 AlexNet이 등장하면서 부터였는데
이미지넷(100만장)을 1000개의 클래스로 분류하는 문제 등이있다.
우수한 성적을 거둔 모델로 VGG,GoogLeNet, ResNet이 유명하다.

### vGG

VGG는 구성이 간단해서 응용하기 좋은데 16층을 두고 풀링층을 통과하며 크기가 절반으로 줄어드는 처리가 반복된다.

### GoogLeNet

깊이뿐만 아니라 너비도 깊어지고 있는데 이를 인셉션 구조라 하며
크기가 다른 필터,풀링을 여러개 적용하여 결과를 결합한다.
그리고 1X1크기의 필터 합성곱을 많은곳에 사용하는데 이는 채널쪽의 크기를 줄이고 매개변수 제거 및 고속처리에 기여한다.

### ResNet

마이크로 소프트의 팀이 개발한 Network로
층이 지나치게 깊으면 성능이 떨어지고 학습이 잘되지않는 것을
skip connection을 도입해서 해결했다.

스킵연결의 핵심은 input data를 합성곱 계층을 건너뛰어 출력에 바로 더하는 구조이다.(왜냐하면 역전파때 스킵연결이 신호감쇠를 막아주기때문이다.)

> 스킵연결은 input data를 그대로 흘리는 것으로 역전파시에도 상류의 기울기를 그대로 하류로 흘려보낸다.(아무런 수정도 하지 않고.)

이렇게 이미지넷에서 제공한 dataset으로 학습한 가중치값들을 실제 제품에도 활용하는데. 이를 전이학습이라 하고 이는 학습된 가중치를 다른 신경망에 복사한다음 그상태로 재학습을 수행하는 것을 말한다.
이는 학습할 dataset이 적을때 유용하다.(fine tuning)

### 더 빠르게 (딥러닝 고속화)

최근 딥러닝 프레임웤 대부분을 GPU(Graphics PRocessing Unit)을활용해
대량의 연산을 고속으로 처리한다.

AlexNet의 경우 forward 처리(순전파) 에서는 합성곱계층 처리시간이 GPU에서 95퍼 CPU에서 89퍼에 이른다.

GPU는 원래 그래픽 전용보드에 이용했는데 이를 범용 수치연산에도 이용하기 시작했고. GPu는 병렬 수치연산을 고속으로 처리할수있따.

딥러닝에서 수행하는 단일곱셈-누산(혹은 행렬끼리의 곱)은 병렬연산으로 GPU의 특기이다.
한편 CPU는 연속적인 복잡한 계산을 잘 처리한다.

GPU는 주로 NVIDIA와 AMD 두 회사가 제공하는데, 딥러닝과 더 친한 쪽은 NVIDIA쪽이다.
실제로 대부분의 딥러닝 프레임웤은 엔비디아 GPU에서만 혜택을 받을수있고

엔비디아 GPU컴퓨팅용 통합 개발환경인 CUDA를 사용하기 때문이다.
cuDNN또한 CUDA위에서 동작하는 lib이다.

### 분산학습

이처럼 딥러닝을 학습하는데 걸리는 시간을 단축하고싶다는 요구가 생겨난다.
따라서 수평확장(분산학습)이 등장하게된다.
최근에는 다수의 GPU와 computer를 이용한 분산학습을 지원하는 deeplearning framework가 생겨나고있는데
그중에서 구글의 텐서플로와 CNTK (computational Network Toolkit)은 분산학습에 초점이 맞춰져있따.

큰 data center의 low latency, high throughput 네트워크 위에서 이 프레임워크드르이 분산학습은 놀라운 효과가 있다.

#### 연산 정밀도와 비트줄이기

계산 외에도 memory용량과 버스 대역폭등이 DL고속화의 병목지점이 될수잇는데
메모리용량은 가중치 매개변수와 중간 data를 메모리에 저장하는것,
버스 대역폭은 GPU의 버스를 흐르는 data가 많아져 문제가 생가는것이 있는데
따라서 데이터의 비트수는 최소로 만드는 것이 좋다.

다행히 DL은 높은 수치정밀도를 요구하지 않는다
이로써 이미지에 노이즈가 섞여있어도, 혹은 data를 퇴화 시켜도 출력에 영향을 적게준다.

따라서 32비트 단정밀도, 64비트 배정밀도의 포맷이있지만
16비트 반정밀도만 사용해도 문제가 없다한다.(파스칼 아키텍쳐는 이 포맷을 지원(2016))

파이썬의 넘파이도 16비트를 지원하는데 이를 사용해도 정확도가 떨어지지않는것을 확인할수있고, 다만 스토리지가 16비트지 연산이 16비트는 아니다.

최근에넌 가중치와 중간데이터를 1비트로 표현하는 Binarized NN방법도 등장.

### 딥러닝 활용

사물인식, 이미지, 음성,자연어등의 분야가있는데

- 사물검출
  R-CNN(Regions with CNN)을 이용하여 이미지 내 사물을 검출한다.
  후보영역을 추출하고 CNN계산을 하고 영역을 분류하는 작업이다.

이때 후보영역 추출에는 컴퓨터 비전분야가 개입이되는데
R-CNN논문에는 Selective Search기법이 이용되어있따.
이 영역 추출까지 CNN으로 처리하는 Faster R-CNN기법도 등장했다

- 분할
  이미지를 픽셀 수준에서 분류하는 문제, 픽셀단위로 채색된 supervised data를 사용해서 학습.
  가장 단순한 방법으로는 모든 픽셀을 추론하는건데.
  너무 시간이 오래걸린다. 따라서 FCN(Fully Convolutional Network) 가 고안되었고
  한번의 forward 처리로 모든 픽셀 클래스를 분류해주는 기법이다.
