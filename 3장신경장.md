## 신경망

퍼셉트론으로는 복잡한 함수도 표현할수있는데 여전히 가중치는 사람이 수동으로 해야한다.
신경망은 퍼셉트론의 이러한 단점 --> 수동으로 가중치를 설ㅈ어하는 부분을 해결해준ㄷ.

즉. 신경망은 가중치 매개변수의 적절한 값을 data로 부터 자동으로 학습하는 능력이 있다.

신경망은 퍼셉트론과 유사한데
입력층, 은닉층, 출력층으로 구성되어있다. (이때 은닉층의 뉴런은 사람의 눈에는 보이지 않아서 은닉층이다.)

퍼셉트론에서는 우리가 x1,x2노드가 한개의 node로 입력이 들어가고 판단하는 모양이었는데
판단하는 식이 `x1*w1+ x2*w2 + b`로 바뀐만큼

입력하는 노드를 x1,x2가 아닌 그이상 x1,x2,b로 추가해주고 b의 입력은 항상 값이 1이고 가중치를 b로 설정해주도록 하자.
그리고 조금더 노드를 파헤쳐서 활성화 함수라는것을 명시적으로 나타내 주게 되는데 이부분이 퍼셉트론에서 신경망으로 나아가는 모양이다.

* 활성화 함수
활성화 함수는 다름이 아니라 x1,x2의 입력을 받은 node가 입력값들의 총합을 0보다 큰지,작은지 판단해서 출력을 결정짓게 되는데 이 판단과 결정을 하는 부분을 활성화 함수라고 한다.(일반적으로는 노드안에 숨겨놓고 안보여 주지만 상세하게 설명을 하려 한다면 입력을 받는 노드에서 상세히 활성화함수를 상셓 기술해주기도한다.)
> 일반적으로 단층퍼셉트론은 계단함수를 활성화 함수로 사용하고 다층퍼셉트론은 시그모이드 함수와 같은 매끈한 모양의 함수를 활성화 함수로 사용한다.

우리가 지금까지 사용했던 임계값을 기점으로 0,1 이렇게 출력이 바뀌는 것은 계단함수라고 한다.(그리고 이것이 퍼셉트론이다)
반면 활성화 함수로 계단함수가 아닌 다름 함수를 사용하는 것이 신경망이다.

#### 시그모이드 함수
시그모이드 함수는`h(x) = 1/(1+exp(-x))`의 모양을 가진 함수인데,
신경망에서는 활성화 함수로 시그모이드 함수를 이용하고 변환된 신호를 다음 뉴런으로 전달하게된다.

시그모이드 함수는 특정 임계점을 경계로 0,1이렇게 변화시키는 것이 아니라 매끄러운 값을 출력해주므로 신경망학습에 좋다.
(그래도 입력값이 중요하면 ==> 크며) 1에 가까운 값을 출력하고 입력값이 덜 중요하면 0에 가깝게 처리해준다.

또한 두개의 함수 모두다 비선형함수인데 신경망에서는 활성화함수로 비선형함수를 사용해야한다
왜냐면 활성화함수로 선형을 사용하면 신경망의 층을 깊게하는 의미가 없어진다.
(퍼셉트론도 활성화함수는 계단함수를 사용한다)

#### ReLU함수
최근 많이 사용되고있는 활성화 함수로
입력이 0을 넘으면 그대로 출력하고 0 이하면 0을 출력하는 함수이다.
(ReLU의 기원은 정류된이라는 의미로 0 이하면 -를 차단하는 그런 의미로 ReLU함수라고 부르게 됐다.)

### 뉴런의 입력값 연산하기

입력값 x와 b 그리고 가중치 w를 계산해보면 A = XW + B 이렇게 간소화 할수 있다.
그리고 행렬간의 계산은 모양을 맞춰줘야지 된다.


이렇게 0층,1층,2층 이런식으로 진행되는 것을 순방향이라고 하고 우리는 앞으로 역방향으로도 진행시켜줄수 있다.



### 출력층 설계하기
신경망은 분류(not continuous)와 회귀(continuous) 모두 이용할수있다.
일바넉으로 회귀에는 항등함수를 분류에는 소프트맥스 함수를 사용한다.

* 항등함수
항등함수의 경우 입력값이 그냥 출력값이 되므로 그냥 구현하면 된다.

* 소프트맥스
소프트맥스 함수의 경우 확률로 나타내지는데 특정 출력 뉴련/ 모든 출력뉴런의 합 의 모양으로 나타낸다.
따라서 각 출력층은 모든 입력신호로부터 영향을 받는다.

소프트맥스 함수를 구현할때는 조심해야 할 부분이 오버플로인데 그 이유는 지수함수로 이루어져 있는만큼 값의 증가가 급속도로 이루어지기 때문이다.
따라서 식의 변형을 거쳐 정규화를 해줘야하는데 일반적으로 입력값중 최댓값을 이용한다.

이때 소프트 맥스를 사용한 출력값은 총합이 1이 되는데 이는 확률로 해석할수 있게 한다.

소프트 맥스 함수는 단조증가 함수이므로 각 입력 data간의 크기순서가 변하지 않는데 따라서 현업에서는 함수 계산에 드는 자원낭비를 줄이고자
추론단계(test)에서의 소프트맥스함수는 생략한다. 대신 학습(train)단계에서의 소프트 맥스함수는 사용해야한다.

### 출력층의 뉴런수 정하기
문제에 맞게끔 출력 뉴런수를 정하면 되는데 일반적으로 분류하고 싶은 클래스의 수로 설정한다.
ex) 숫자를 분류하는 문제 ==> 0~9 10개의 뉴런을 출력


* normalize : 정규화 ( 0.0~1.0)사이의 값으로 하는 여부 (이와같이 특정 처리를 가하는 것을 전처리라고 한다)
* flatten : 입력 받은 값을 1차원 배열로 변환할지
* one-hot : [0,0,1,0,0,0] 과 같은 방식으로 정답인 원소만 1로 만들어주고 나머지는 0으로 처리하는 방법.
* argmax() : 배열중에 가장 값이 큰 원소의 인덱스를 구한다 ==> 해당 숫자가 될 확률이 가장 높다는 뜻이다.
> Pickle
> 프로그램 실행중 특정 객체를 파일로 저장하는 기능으로 저장해둔 pickle파일을 로드하면 실행 당시의 객체를 즉시 복원할수 있다. 마치 캐싱과 같은 기능

이미지를 flatten으로 1차원 배열로 만든경우 다시 이미지를 표시할때 reshape()로 원래 shape로 돌아가서 그려줘야한다.

> 현업에서도 신경망(딥러닝)에 전처리를 활발히 사용==>전처리를 사용해서 능력개선 및 학습속도 향상을 한다.
> 데이터 백색화 및 평균과 표준편차를 이용하는 방법등 다양한 방법으로 정규화를 해줄수 있다

## 배치
입력데이터를 하나씩 입력하는게 아니라 묶음 단위로 넣어줄수도 있는데 이를 batch배치라고 한다.
* 수치계산 라이브러리들이 큰 배열을 처리하는데 효과적으로 구성되어 있고
* 무엇보다도 매번 데이터를 읽어오게 되면 I/O때문에 CPU사용량이 많아져서 시간이 오래걸리므로 한번에 많은 DATA를 읽어 부하를 줄여준다.