- 가중치 매개변수의 최적화 방법,
- 매개변수 초깃값, 하이퍼파라미터 설정방법
- 오버피팅의 대응책인 가중치 감소와 드롭아웃(정규화방법)
- 배치 정규화

#### 매개변수 갱신

지금까지는 매개변수의 최적값을 SGD(확률적 경사하강법)을 통해서 찾아나갔다.
이런 SGD의 단점을 알아보고 다른 최적화기법을 사용해보자

SGD는
`W = W - lr*gradient` 로 표현할수 있다. 직관적으로 보면 기울어진 방향으로 일정거리 가겠다는 방법.

의사코드로 SGD를 사용하는 법을 작성해보면

```python
network = TwoLayerNet()
optimizer = SGD()

for i in range(10000):
    ...
    x_batch, t_batch = get_mini_batch(...) #미니배치
    grads = network.gradient(x_batch, t_batch)
    params = network.params
    optimizer.update(params, grads) #여기서 매개변수 갱신을 해준다.
```

이처럼 대부분의 DL 프레임워크에서는 최적화기법과 같은것을 모듈화 시켜놓고 사용하기 편하게 해준다(ex. Lasagne)

#### SGD

SGD는 단순하고 구현하기 쉽지만 문제에 따라서는 비효율적이다.
특히 anisotropy function(방향에따라 기울기가 달라지는 함수)에서는 탐색 경로가 비효율적이게 나올수있다.

### Momentum (모멘텀)

운동량을 뜻하는 만큼 물리와 관계가 있는데

`v = av - lr*gradient` 이떄 av는 아무 힘이없을때 하강을 나타낸다.(중력가속도등)
`W = W + v`
즉 기울기방향으로 가속이 붙는다는 것을 고려한 식이다.

이렇게 구현하면 공이 바닥을 구르듯 움직이기때문에 SGD에 비해서 경로가 효율적이다.(기울기의 영향을 적게받는 축으로 고정된 힘을 받고있다고 가정하기때문에)

### AdaGrad

신경망학습에서는 학습률(lr) 값이 중요하다.
너무 작으면 학습시간이 길어지고, 너무 크면 발산으로 튕겨저 나간다

이 학습률을 정하는 기술로는 learning rate decay 가 있는데
학습을 진행하면서 점차 줄여가는 방법이다.
이때 매개변수 전체의 학습률을 일괄적으로 낮추는 방법이 있는데
이를 발전시킨게
AdaGrad(각각의 매개변수에 맞춤형 값을 만든다)

`h = h+gradient**2`
`W = W - lr\*1/n^1/2*gradient`

이처럼 h는 기울기값을 제곱해서 계속 더하여
매개변수를 갱신할때 기울기가 큰 원소(많이 움직인)에 대해서는 학습률을 낮게 적용한다.
즉 매개변수 원소마다 다르게 적용한다

그리고 계속 더해가는만큼 학습을 진행할수록 갱신 강도가 약해진다.
그래서 이를 개선한 기법이 RMSPRop라는 방법이 있다.
이 방법은 과거의 기울기는 점점 잊고 새로운 기울기를 크게 받영하는데
이를 지수이동평균이라 하여 과거 기울기의 비중을 낮춰 준다.

따라서 갱신 경로를 확인해보면 이동이 큰 y축 방향으로는 이후부터 속도를 줄여서 이동하기 시작하고 x는 그에 비해 영향을 덛 받는 것을 알수 있다.

### Adam

모멘텀처럼 물리적인 법칙을 이용하는 것과 더불어서
매개변수의 원소마다 적응적으로 갱신정도를 조정하는 AdaGrad 를 융합해서 만들었다.
2015년 제안되었는데 두방법을 조합한만큼 효율적인 탐색과 더불어 하이퍼파라미터의 편향 보정이 진행된다.

이처럼 Adam도 그릇바닥을 구르듯 움직이는데, 모멘텀보다 지그재그로 움직이는게 적다.
AdaGrad를 적용한 효과이다.

> Adam은 하이퍼파라미터를 3개 설정하는데 하나는 지금까지의 학습률,(lr), 나머지 두개는 일차 모멘텀용 계수(0.9)와 이차 모멘텀용 계수(0.999)를 뜻한다.

위의 4가지 방법이 있지만 어떤 문제인지에 따라서, 하이퍼파라미터를 어떻게 설정하는가에 따라서도 달라진다.(신경망의 구조, 총 깊이에 따라서도)

지금으로써는 SGD 와 Adam을 많이 사용하고 있다.
그치만 더 일반적으로 SGD보다 Adam이 더 빠르게 학습하고 최종 정확도도 높게 나타난다.

### 가중치의 초깃값

가중치의 초깃값을 무엇으로 설정하느냐에 따라 학습의 성패를 가르는 일이 많다.

- overfitting을 억제해 범용 성능을 높이는 방법인 가중치 감소
  가중치 값을 작게해서 오버피팅이 일어나지 않도록 하는 것.

만약 그럼 초깃값을 0으로 설정하면 어떻게 될까?
==> 학습이 올바로 이뤄지지 않는다.

초깃값을 모두 0으로 해서 안되는 이유는?(정확히는 가중치를 균일값으로 설정하면 안되는 이유) 오차역전파법에서 모든 가중치의 값이 똑같이 갱신되기때문이다.

즉 순전파로 갈때는 두번째층의 뉴런에 같은 값이 가는데 이는 역전파로 돌아올때도 가중치가 모두 똑같이 갱신된다는 말이다. ==> 즉 갱신을 거쳐도 같은 값을 유지한다.(따라서 초깃값이 대칭적인 균형을 무너뜨리면서 무작위로 설정해야한다.)

- 은닉층의 활성화값(활성화함수의 출력데이터)의 분포를 관찰해보자
  활성화 함수로 시그모이드 함수를 사용하는 5층 신경망에 무작위로 생성한 input data를 넣어 각 층의 활성화 값의 분포를 확인해보자

일반적인 sigmoid를 사용해서 학습을 하게되면(표준편차 1) 활성화 값들이 0과 1에 치우쳐 분포가 되어있기때문에 역전파의 기울기 값이 점점 작아지다가 사라지는
gradient vanishing문제가 발생한다.

따라서 우리는 이러한 활성화값들이 특정 값에 치우친게 아닌 고르고 다양하게 분포해야지 올바른 학습을 할수 있다.
치우쳐서 같은 값을 출력한다는 것은 뉴런이 많아도 같은 값을 출력한다는 것이므로 의미가 없다.(표현력이 제한된다)

이러한 문제를 해결하기위해서 가중치 초깃값을 Xavier 초깃값을 이용해서 설정해보자
(Xavier 초깃값은 일반적인 딥러닝 프레임워크에서 표준적으로 이용하고 있다.)
(Caffe 프레임워크는 가중치 초깃값을 설정할떄 args 로 xavier로 지정할수있다.)

주된 목적은 각층의 활성화값을 광범위하게 분포시키는것이고
앞 계층의 노드가 n개이면 표준편차를 1/n^1/2로 사용하면 된다는 결론이 나온다
(논문에서는 다음층의 노드도 고려했지만 단순화해서 사용한다.)

> 단 이때 Xavier 초깃값은 활성화 함수가 선형인것을 전제로 한다(sigmoid 과 tanh는 좌우 대칭이라 중앙 부근에서 선형이다.)

### ReLU를 사용할때 초깃값

ReLU를 이용할때의 초깃값을 He 초깃값이라 한다.
그리고 초깃값은 앞층의 노드가 n일 때 `(2/n)^1/2` 로 주어진다.
(직관적으로 생각해보면 ReLU는 0이하가 0이므로 이전의 sigmoid에 비해서 2배의 값이 필요하다)
이를 통해서 가중치값을 초기화하면 층이깊어져도 활성화값들이 치우치지 않고
기울기 소실 문제도 없어진다.(따라서 역전파때도 적절한 값이 나올것이다.)

따라서 초깃값의 설정은 정말 중요한 문제이다.(신경망 학습의 성패를 결정짓는 경우도 많다.)

### 배치 정규화

위의 방법들은 가중치의 초깃값을 잘 설정하여 활성화값을 퍼뜨린 반면
이번에는 강제로 활성화값을 퍼뜨려 보도록 하자.

- 학습을 빨리 진행할수있다.
- 초깃값에 크게 의존하지 않는다
- 오버피팅을 억제한다

input data --> Affine --> `Batch Norm` --> ReLU --> Affine --> `Batch Norm` --> ReLU --> Affine --> Softmax

배치정규화는 미니배치를 단위로 정규화 해준다.(평균이 0, 분산이 1)
그리고 정규화된 data에 확대와 이동을 해줄수있는데
`y = Υx + β`
Υ가 확대 β가 이동을 담당한다.
배치 정규화의 역전파는 궁금하면 프레드릭 크레저트의 블로그를 찾아보자

이렇게 배치정규화를 시키면 거의 모든경우에서 학습진도가 빨라지는데(무조건은 아니다) 이렇게 속도 뿐만아니라 가중치 초깃값에 크게 의존하지 않아도 된다.

### 바른 학습을 위해

- 오버피팅 : 신경망이 훈련데이터에만 지나치게 적응하여 그 이외의 data에는 제대로 대응하지 못하는 상태,

오버피팅은 주로

- 매개변수가 많고 표현력이 높은모델
- 훈련데이터가 적은 모델

의 경우에서 일어나게 된다.

#### 해결방법

- 가중치 감소
  큰 가중치에 대해서 패널티를 부과해 overfitting을 억제하는 방법
  (오버피팅은 가중치의 매개변수가 커서 발생하는 경우가 많기때문)
  가중치의 L2 norm(가중치의 제곱 norm)을 손실함수에 더한다.
  가중치를 W라 하면 L2 norm에따른 가중치 감소는 `1/2λW^2`가 되고 이를 손실함수에 더한다

가중치감소는 모든 가중치각각의 손실함수에 `1/2λW^2`를 더하는데 역전파에 따라 기울기를 계산할때는 미분한 λW를 더한다

> norm의 종류에는 L2 norm, L1 norm, L∞ 가 있다.

- 드롭아웃

신경망 모델이 복잡해지면 가중치 감소만으로는 대응하기 어려워 지는데, 이럴때는 Dropout이라는 기법을 이용한다.

Dropout은 뉴런을 임의로 삭제하면서 학습하는 방법,
훈련때는 data를 흘릴때마다 뉴런을 무작위로 삭제하고 시험할때는 모든 뉴런을 살려서 전달한다.
Dropout의 효율적인 구현이 궁그하면
Chainer 프레임워크의 드롭아웃 구현을 참고해보자

드롭아웃을 이용하면 표현력이 높아지면서도 overfitting을 억제할수도 있다.

> ML에서는 앙상블을 애용하는데 개별적으로 학습시킨 여러 모델의 출력을 평균내어 추론하는 방식이다.
> 이는 드롭아웃과도 비슷하다고 볼수있는데, 드롭아웃으로 매번 삭제하는것을 새로운 네트워크라고 생각해주면 앙상블과 같은 모양이 되기 때문이다.

### 적절한 하이퍼파라미터 찾기

- 검증데이터
  **하이퍼파라미터를 검증할대는 testdata를 사용해서는 안된다.**
  ==> 왜냐하면 하이퍼파라미터가 test data에 오버피팅 되기 때문이다.
  따라서 전용테스트 데이터를 마련해놔야하는데 이를 검증data(validation data)라고 부른다
  (일반적으로는 훈련데이터의 20%정도를 검증데이터로 빼서 검증한다.)

* 최적화 과정
  핵심은 최적값이 존재하는 범위를 조금씩 줄여나가는 것이다.
  줄여나가면서 그 범위에서 무작위로 하이퍼파라미터 값을 골라낸(샘플링) 후 그 값으로 정확도를 평가한다.

이 작업에서는 그리드 서치와 같은 규칙적인 탐색보다 무작위로 샘플링해서 탐색하는게 더 좋은 결과를 낸다.

이때의 범위는 대략적으로 지정하는것이 효과적인데
0.001 ~1,000 사이와 같이 10의 거듭제곱 단위로 범위를 지정한다.
그리고 이를 로그스케일로 지정한다고 한다.

하이퍼 파라미터 최적화에는 오랜시간이 걸릴수있따. 따라서 나쁠것 같은 값은 일찍 포기하고, 에폭값을 작게하여 1회 평가에 걸리는 시간을 단축하자

1. 하이퍼파라미터 값의 범위를 설정한다
2. 설정된 범위에서 하이퍼파라미터의 값을 무작위로 추출
3. 1단계에서 샘플링한 하이퍼파라미터 값을 사용해 학습하고, 검증데이터로 정확도 평가한다( 에폭 단위작게설정)
4. 1단계와 2단계를 특정횟수 반복하여 하이퍼파라미터의 범위를 좁혀간다

> 조금더 세련된 기법으로는 베이즈 최적화 기법이있다.(베이즈 이론을 중심으로 하여 더 엄밀하고 효율적으로 최적화를 수행한다.)

```python
weight_decay = 10 ** np.random.uniform(-8,-4)
lr = 10** np.random.uniform(-6,-2)
```

위와같이 설정하여 반복하다가 검증데이터에 대한 정확도, 학습이 잘 진행되고있는 부분까지 파악하여 그 부근 범위의 하이퍼파라미터를 다시 학습하기 시작한다.
