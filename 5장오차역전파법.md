4장에서는 수치미분을 이용해서 구현을 했는데
수치미분은 단순하고 구현이 쉽지만 시간이 오래걸린다.
따라서 오차역전파법(역전파)(backpropagation) 을 이용해서 구현해보도록하자

# backpropagation

수식을 통한 이해와 계산 그래프를 통한 이해가 있다.

### 계산그래프

계산과정을 그래프 자료구조로 나타낸것으로
node 와 edge 로 표현한다.

사과 --100원--> X2(노드) --200--> x1.1(노드) --220-->

> 이처럼 노드안에 연산내용이 들어가고 edge를 통해서 data가 움직인다.

이때 그래프에서 계산을 왼쪽에서 오른쪽으로 진행하는 것을 순전파라 한다.
반대는 역전파이고 미분을 계산할때 중요한 역할을 한다.

계산그래프는 국소적 계싼을 통해서 최종결과를 도출해 낸다.
각 노드는 자신과 연관된 계산외에는 신경쓸필요가 없다.

- 국소적 계산을 통해 각문제에 집중함으로 단순화시킬수있다.
- 중간 계산 결과를 모두 보관할수 있다.
- 역전파를 통해 미분을 효율적으로 계산할수 있다.
  순전파의 반대 화살표를 굵은선으로 그리고 아래에 미분값을 적어 비례를 볼수있다.

### 연쇄법칙

역전파가 국소 미분을 전달하는 원리는 chain Rule에 따른것이다.

합성함수 : 어러함수로 분리해 나타낼수있는 함수
ex) z = (x+y)^2
z = t^2
t = x+y

합성함수의 미분은 합성함수를 구성하는 각 함수의 미분의 곱으로 나타낼수있다.

이를 미분에도 적용시켜 줄수있다. 연쇄법칙은 어렵게 생각할 필요없이
내가 필요한 함수를 분할해서 여러 함수의 곱으로 나타낼수 있는것이다.

### 역전파

덧셈의 역전파는 그대로 전달해주기때문에 순전파의 입력신호가 필요없었는데
역전파는 다른 입력신호의 값을 곱해야하므로 순방향 입력신호의 값이 필요해서 저장해둔다.

### 단순한 계층 구현하기

곱셈노드 : MulLayer
덧셈노드 : AddLayer

> 다음 절에서느 신경망을 구성하는 계층을 class로 구현한다. 계층이랑 신경망의 기능단위.

모든 계층은 forward()(순전파)와 backward()(역전파)라는 공통의 method, 인터페이스를 갖도록 구현할것이다.

- 곱셈계층 : 곱셈계층은 역전파로 돌아갈때 입력신호의 상대값이 필요하므로 저장해놓지만
- 덧셈계층 : 덧셈계층은 역전파로 돌아갈때 그냥 보내줘도 되니까 init으로 x와 y를 따로 초기화해서 저장해주지 않는다.

이렇듯 계산그래프를 이용하면 복잡한 미분도 손쉽게 할수 있다.

### 활성화 함수 계층 구현하기

신경망을 구성하는 계층을 각각 하나의 클래스로 구현한다.

#### ReLU 계층 구현하기

- ReLU의 수식

  - y = x (x>0)
  - y = 0 (x<=0)

- ReLU의 미분식
  - dy/dx = 1 (x>0)
  - dy/dv = 0 (x<=0)

> 따라서 위의 식을 보면 순전파의 입력이 0이하면 역전파로 돌아갈때 하류로 신호를 보내지 않는다. 입력이 0이상이면 그대로 전해주는데

```python
class Relu:
    def __init__(self):
        self.mask = None

    def forward(self, x):
        # x는 넘파이 배열로 들어오는데 x가 0보다 작거나 같은 index를 True로 하여 배열생성
        self.mask = (x <= 0)
        out = x.copy()
        out[self.mask] = 0  # 0보다 작거나 같은 애들이 True이므로 그 index에 해당하는 ele를 0으로 바꿈

        return out

    # 역전파는 순전파때 만든 mask를 이용해서 True인 index에 대해서 dout을 0으로 전달한다.
    def backward(self, dout):
        dout[self.mask] = 0
        dx = dout

        return dx
```

#### sigmoid

y = 1 / (1+exp(-x))

연산 노드의 순서를 보면

x -> exp -> + -> / 식으로 나뉘어진다.
역전파를 순서대로 오른쪽->왼쪽으로 역행하면서
sigmoid의 역전파를 한번 구현해보자

1. `/` 노드 미분( y = 1/x )
   dy/dx = -(1/x^2)
   = -y^2

따라서 해당 노드를 지나면 상류에서 흘러들어온 값에 -y^2을 곱해서 하류로 보낸다

2. `+` 노드
   별다른 작업없이 하류로 보낸다
3. `exp` 노드(y=e^x)
   dy/dx = e^x

따라서 1에서 곱해진 값에 더해서 e^x를 곱해서 전해준다.

4. `x` 노드
   순전파 때 입력받은 값을 서로 바꿔 곱하므로
   여기서는 -1이 순전파때 다른 입력으로 주어졌으므로 -1을 곱하면 된다.

최종출력 :
`dL/dy(y^2*e^-x)`
입력값인 x와 출력값인 y만으로 계산이 되고 중간과정을 모두 묶어 sigmoid node라는 역전파 노드 하나로 대체해버릴수 있따.
(이럴때 중간과정을 생략하여 보다 효율적이기도 하지만 그룹화 해서 외부에 내용을 노출하지 않는것도 중요포인트다.)

`dL/dy(y^2*e^-x)` == `dL/dy(y*(1-y))`
위의 식으로 바꿔서 계산할수 있는데
이는 출력 y만으로도 계산할수 있다는 말이 된다.
그리고 이를 이용해서 `backward`를 구현한다.

#### Affine/Softmax 계층 구현하기

신경망의 순전파에서 수행하는 행렬의 곱은 기하학에서
affine transformation이라고 한다.

따라서 Affien변환을 수행하는 처리를 affine계층으로 구현한다.

이때 역전파를 구할때는 행렬에 대응하는 차원의 원소수가 일치하도록 곱을 조립하여 구한다.

##### 배치용 Affine 계층

#### Softmax - with - Loss 계층

출력층에서 사용하는 softmax함수에 대해서 알아보자
softmax는 입력값을 정규화(이는 총합이 1로 확률로 나타난다)해서 출력한다.

> 신경망을 학습시킬때는 softmax함수로 정규화 해줘야하는데, 신경망을 이용해 추론하는 과정에서 답을 하나만 내는 경우에는 가장 높은 점수만 알면 되므로 굳이 softmax함수를 쓸 필요는 없다.

softmax 계층의 역전파는 (y1-t1, y2-t2)와 같은 방식으로 말끔한 결과를 보여주는데 이떄의 y는 softmax계층의 출력이고 t는 정답레이블이므로
출력과 정답의 차분이 앞계층으로 전해지는 것이다.

신경망의 목적은 신경망의 출력(softmax의 출력)이 정답과 가까워지도록 가중치 매개변수를 조정하는것이다.
따라서 효율적으로 앞으로 전달해야한다.

역전파로 위와같은 모양이 나오는 이유는 교차 엔트로피 오차함수가 이렇게 설계가 되었기때문이다.
이와 같은 원리로 항등함수의 손실함수로 오차제곱합을 하는 이유도 말끔히 떨어지게 설계해서이다.

### 오차역전파법 구현

이러한 계층들을 조합하면 레고조립하듯 신경망을 구축할수있다.
오차역전파법이 등장하는 단계는 기울기산출 단계에서 적용해줄수있는데
기존의 수치미분에 비해서 효율적이고 빠르게 해줄수있다.

이번장에서 각 계층을 모듈화해서 만들어놨기때문에 여러층이더라도 효율적으로 구현할수 있다.

#### 기울기 검증하기

- 수치미분을 써서 구하는 방법
- 해석학적 방법(오차역전파법)

1번과 2번의 방법이 있는데 2번을 구현해놓으면 1번은 성능상 쓸모가 없을지도 모르지만 1번방법이 구현이 쉬우므로 우리는 1번을 통해서 2번을 검증하도록 한다.

정리
계산그래프의 순전파는 통상의 계산을 하고
계산그래프의 역전파는 각 노드의 미분을 구할수 잇따
신경망 구성요소를 계층으로 구현해 기울기를 효율적으로 계산 할수있다.(오차역전파법)
