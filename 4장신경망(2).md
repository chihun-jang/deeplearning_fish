> 학습 : 훈련데이터로 부터 가중치 매개변수의 최적값을 자동으로 획득하는 것, 신경망에서는 cost function을 기반으로 한다.

실제 신경망의 매개변수는 정말 많은데 이를 수작업으로 설정하는것은 불가능하다.
따라서 신경망 학습에 대해서 알아보자.

> 퍼셉트론도 선형분리문제는 학습으로 풀수있지만 비선형 분리문제는 학습할수 없다.

우리는 DATA를 보고 직관과 경험을 통해서 규칙성을 발견할수있는 반면 프로그램은 feature를 추출하고 pattern을 ML로 학습하는 방법이있다.
이때의 feature은 input data에서 중요한 data를 추출할수 있게 설계된 변환기인데
img의 경우 verctor로 기술하고 computer vision의 경우 SIFT, SURF, HOG등의 특징을 많이사용한다.
이렇게 data를 vector로 변환하고 vector를 지도학습의 분류방법인 SVM,KNN등으로 학습한다.

이때 규칙을 찾아내는 역할은 기계가 하는데 이러한 img를 vector로 변환하는 과정은 사람이 하므로, 적절한 feature를 뽑아내는 ㄱ서도 중요하다.

즉 기계가 학습을 하는 패러다임이 크게 3가지가 있는데

- 사람이 생각한 알고리즘
- 사람이 생각한 feature(SIFT,HOG)로 `기계학습(SVM,KNN)`
- 신경망(딥러닝) - 중요한 특징까지도 기계가 스스로 학습함, 사람의 개입이 없다.

기계학습문제는 DATA를 training DATA와 TEST DATA로 나눠 학습과 실험을 진행한다.
그 이유는 범용적으로 사용할수 있는 모델을 개발하기위해서 데이터를 분리한다
여기서 말하는 범용은 임의의 DATA에 대해서 판별하는 능력이고

이는 DATA set 하나로만 학습과 평가를 하면 신뢰성있는 결과를 얻기 힘들다.
이렇게 하나의 DATASET에 지나치게 최적화 된 상태를 overfitting이라고 한다.

신경망에서는 최적의 매개변수를 찾기위해 cost function을 지표로 사용한다.
그리고 신경망에서는 일반적으로 오차 제곱의 합과 교차 엔트로피 오차를 사용한다.

- 오차제곱합(Sum of squares for Error, SSE)

이때의 오차는 신경망의 출력(y)와 정답 레이블(t)간의 Error이며 k는 data의 차원수를 나타낸다.

> one-hot encoding : 정답이 2일때 [0,0,1,0,0] 이런식으로 정답에 해당하는 index만 1로 표현하는 표기법

- 교차 엔트로피 오차(cross entropy error, CEE)(y:신경망의 출력, t:정답레이블)
  수식 : E = -sum(t_k\*log(y_k))
  위의 식은 실질적인 정답일때의 추정값의 자연로그를 계산하는 식이 된다.
  (왜냐면 정답값은 one-hot encoding으로 1이되고 그때의 추정값은 확률로 나타니기때문이다.)
  즉 교차엔트로피 오차는 정답일때의 출력이 값을 결정하는데 중요한 요소가 된다.

- 미니배치학습
  기계학습 문제는 train set을 사용해서 학습한다.
  정확하게는 train set에 대한 손실함수의 값을 구하고 그 값을 줄여주는 매개변수를 찾아내려한다.
  그런데 이러한 방대한 훈련 data에서 모두 일일이 cost function을 구하려면 비효율적일수 있다(왜냐하면 N만큼 확장을 하고 N으로 나누어 정규화 시켜줘야하기때문에) 따라서 신경망에서도 훈련 data로 부터 일부만 골라서 학습을 수행하게 되는데 이 일부를 `mini-batch`라고 한다.
  6만개의 data가 있으면 그중 100장을 뽑아 100장만을 사용하여 학습하는 것을 말한다. (표본을 뽑아 모집단을 대표하기)

ex) mnist 를 이용해서 실습을 할때
np.random.choice()를 사용해서 무작위로 batch_size만큼 뽑아오자
`np.random.choice(60000,10)`

### 왜 손실 함수를 설정할까

정확도를 높이기 위해 cost function을 설정하는 이유는
미분(기울기,변화율)를 통해서 cost fun을 작게하는 값을 찾아간다.
이때의 미분값은 가중치의 매개변수를 변화시켰을때 손실함수가 어떻게 변하냐를 나타내게 되고,

미분값이 0이 매개변수의 갱신이 멈춘다(따라서 신경망학습에서는 기울기가 0이 되지않는것이 중요하다).

> 정확도를 지표로 삼아버리면 미분값이 대부분의 장소에서 0이 되어 갱신이 멈출수 있다. 왜냐하면 매개변수를 조금 옮기는 것만으로는 정확도의 개선이 이루어 지기 어렵고, 정확도는 상대적으로 분연속적인 모양을 나타내기 때문이다.
> 이러한 이유때문에 계단함수도 활성화 함수로 사용하지 않는다.(매개변수의 변화에따라 손실함수의 변화를 주지 않아 학습이 원할이 이뤄지지 않기때문)

즉 경사법에서는 미분이 학습방향결정의 중요한 역할을 한다.

### 미분

파이썬으로 미분 공식을 나타낸것, h는 작은 값을 임의로 대입해준다.

```python
def numerical_diff(f, x): # 이름은 수치미분으로 해준것.
    h=10e - 50
    return (f(x+h)-f(x))/h
```

그런데 위에서 넣어준 임의의 h 값 10e-50 은 가수가 10이므로 소숫점 49자리 숫자인데 이 숫자는 `rounding error`문제를 일으킨다(소수점 몇자리 이하 반올림으로 계산결과에 오차가 생기는것)

`print(np.float32(1e-50))`
따라서 적당히 10^-4정도만 해주도록 하자

이로써 발생하는 새로운 문제점이있는데 h를 무한히 0으로 좁힐수 없어서
점과 점+증분 사이의 변화율이 아니라 지금은 그저 점과 점+증분사이의 기울기로 나타나는것이 문제다(즉 어느 순간의 순간 기울기가 아니라 점과 점사이의 기울기라는 말)

따라서 이부분은 x+h뿐만 아니라 x-h도 생각을 해주어 차분을 계산해주는데
이를 중심차분 혹은 중앙차분이라고 해준다. (x와 x+h간에는 전방차분값)

```python
def numerical_diff(f,x):
    h = 1e-4 # 0.0001
    return (f(x+h)- f(x-h)/(2*h))
```

> 이와같이 아주 작은 차분으로 미분하는 것을 수치미분(근사치)이라 하고,
> 수식을 전개해 미분하는 것을 해석적으로 미분한다고 한다.(이는 진정한 미분값을 구해준다.)
> 따라서 수치해석학은 해석학 문제에서 수치적 근삿값을 구하는 알고리즘을 연구하는 수학이다.

> 여기서 뜬금없지만 python은 한번 컴파일 되고 실행되니까 컴파일 될때 에러가 있는지 체크해주고 그다음 실행값을 print해주게 되네

### 편미분

변수가 2개이상일때 어느 변수에 대한 미분인가를 구별해서 해주게 되는데 이를 편미분이라 한다.
이때 다른 애들을 상수 취급해주고 수치미분을 적용해주자. 이때의 미분값 또한 특정 장소의 기울기가 되는데 어떤 변수에 포커싱했다는 것만 다를 뿐이다.

그런데 이때의 편미분을 각 변수별로 따로하는게 아니라 모든 변수의 편미분을 벡터로 정리하면 기울기가 된다.

편미분또한 미분과 같이 각점에서의 기울기를 구해줄수있고 이러한 기울기, 벡터들로는 벡터장을 그려줄수있다.
그림 그려진 벡터장으로 확인읋 해보면 기울기(화살표)는 각 지점에서 낮아지는 방향을 가리키는데 이는 해당 방향으로 갈때 함수의 출력값이 줄어드는 것을 의미한다.

### 경사법

신경망은 cost function을 적게하는 매개변수(가중치와 편향)를 학습시에 찾아줘야하는데 실제로 구성되는 손실함수도 복잡하고 매개변수 공간또한 넓어 최솟값을 찾아가기 힘든데 이러한 최솟값을 찾아가는게 경사법이다.
하지만 복잡한 함수에서는 기울기가 가리키는 곳으로 간다해도
극솟값, 최솟값, 안장점으로 인도할수 있고 이대 극솟값과 안장점에서는 최솟값이라고 부르기 힘들다.

하지만 그럼에도 cost 를 줄일수 잇는 방향으로 나아가야 하므로 기울기에 기반한 경사법으로 방향을 설정한다.(신경망 학습에서 일반적으로 많이 사용한다)

> 학습률(learning rate) 학습률이란 한번의 학습으로 얼마만큼 반영하는지 결정하는 것으로 너무 크거나 작으면 최적의값을 찾아가기 힘들다. 이 값을 변경시키면서 올바르게 학습이 진행되는지 체크한다.

```python

def gradient_descent(f, init_x, lr=0.01, step_num=100):
    x = init_x
    for i in range(step_num):
        grad = numerical_gradient(f, x)
        x -= lr * grad
    return x


init_x = np.array([-3.0, 4.0])
print(gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100))

#처음 x값을 3,4로 설정했는데 0.0에 정말 근사한 값으로 다가가는 모습을 확인할수 있다
```

이때 학습률이 너무 크면 튕겨져 나가거나
학습률이 너무 작으면 학습이 제대로 이루어 지지 않아 learning_rate 의 설정도 중요하다.
학습률과 같은 매개변수를 하이퍼파라미터라 하고, 하이퍼 파라미터를 엔지니어가 직접 설정하여 가장 최적의 값을 찾는 과정이 중요하다.

#### 신경망에서의 기울기

신경망 또한 주어진 가중치(W)에 대해서 손실함수 (L)의 기울기를 나타내고 이를 통해서 학습할수 있다.

각 W의 값에대한 기울기가 나오게 되고 W를 어떻게 조작하는 가에따라서 손실함수의 값도 달라질수 있으므로 우리가 원하는 손실함수를 최소화 하는 지점으로 간다.

```python
def f(W):  # 직접적으로 W를 받고있지는 않지만 같은 instance의 W를 사용해서 내부적으로 메서드작업을 해주고있따.
    return net.loss(x, t)
#위의 함수는 아래와 같이 람다함수로도 구현한다.
f = lambda w: net.loss(x,t)

```

### 학습 알고리즘 구현하기

- 신경망 학습의 절차
  - 신경망에는 가중치와 편향이있고 trainset에 적용하도록 조정하는 과정을 train이라한다.
  1. mini-batch : train-data중 표본을 뽑아서 학습한다.
  2. gradient : 배치 값의 cost function을 줄이기 위해 gradient를 구하고 이는 학습 방향을 제시한다.
  3. 매개변수 갱신 : W를 gradient방향으로 조금 갱신
  4. 1~3 반복

위와같이 미니배치로 하강하는 방법을 확률적 경사하강 stochastic gradient descent(SGD) 라한다.

> Epoch 은 하나의 단위, 1Epoch은 학습에서 훈련데이터를 모두 훈련했을경우,
> 예를 들면 10000개의 훈련 data가 잇을때 batch가 100개라면 100회 훈련한것이 1epoch이 된다.
> 따라서 정확도같은경우 1epoch당 계싼을 해주면 좀더 큰 관점에서 볼수 있다.
